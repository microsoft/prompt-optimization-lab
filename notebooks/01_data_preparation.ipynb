{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bef81ed",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5926d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Dict, List, Set\n",
    "import warnings\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization (optional, for data exploration)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d99ef37",
   "metadata": {},
   "source": [
    "# Load Snowflake Views Metadata\n",
    "\n",
    "Load the database schema information including view names, descriptions, columns, and selector keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9a6449d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading Snowflake Views Metadata...\n",
      "üìÅ Source file: ..\\data\\snowflake_view.json\n",
      "------------------------------------------------------------\n",
      "‚úÖ Loaded 21 Snowflake views\n",
      "\n",
      "üìã Sample View Structure (First View):\n",
      "  ‚Ä¢ View Name: Active Deal List\n",
      "  ‚Ä¢ Entity: ACTIVE_DEAL_LIST_VW\n",
      "  ‚Ä¢ Description: This view contains a list of all deals that are available for consideration and previously closed de...\n",
      "  ‚Ä¢ Columns: 57 columns\n",
      "  ‚Ä¢ Selector Keywords: 759 keywords\n",
      "\n",
      "üìù All View Entities:\n",
      "   1. ACTIVE_DEAL_LIST_VW            (Active Deal List)\n",
      "   2. INVESTMENT_KPI_VW              (Investment KPI View)\n",
      "   3. MIC_KPI_VW                     (MIC KPI View)\n",
      "   4. PLATFORM_KPI_VW                (Platform KPI View)\n",
      "   5. BUSINESS_UNIT_KPI_VW           (Business Unit KPI View)\n",
      "   6. BUSINESS_UNIT_BY_SECTOR_VW     (Business Unit Sector View)\n",
      "   7. MIC_BY_INVESTMENT_CLASS_VW     (MIC Investment Class View)\n",
      "   8. PLATFORM_BY_INVESTMENT_CLASS_VW (Platform Investment Class View)\n",
      "   9. BUSINESS_UNIT_BY_INVESTMENT_CLASS_VW (Business Unit Investment Class View)\n",
      "  10. MIC_BY_SECTOR_VW               (MIC Sector View)\n",
      "  11. PLATFORM_BY_SECTOR_VW          (Platform Sector View)\n",
      "  12. MIC_BY_ASSET_CLASS_VW          (MIC Asset Class View)\n",
      "  13. BUSINESS_UNIT_BY_ASSET_CLASS_VW (Business Unit Asset Class View)\n",
      "  14. MIC_BY_REGION_VW               (MIC Region View)\n",
      "  15. PLATFORM_BY_REGION_VW          (Platform Region View)\n",
      "  16. BUSINESS_UNIT_BY_REGION_VW     (Business Unit Region View)\n",
      "  17. MIC_BY_COUNTRY_VW              (MIC Country View)\n",
      "  18. PLATFORM_BY_COUNTRY_VW         (Platform Country View)\n",
      "  19. BUSINESS_UNIT_BY_COUNTRY_VW    (Business Unit Country View)\n",
      "  20. ALLOCATION_VW                  (Allocations View)\n",
      "  21. CONSUMPTION_VW                 (Consumption View)\n",
      "\n",
      "============================================================\n",
      "üìä Total Views Loaded: 21\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "SNOWFLAKE_VIEWS_FILE = Path(\"../data/snowflake_view.json\")\n",
    "OUTPUT_DIR = Path(\"../data/processed\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üîç Loading Snowflake Views Metadata...\")\n",
    "print(f\"üìÅ Source file: {SNOWFLAKE_VIEWS_FILE}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    with open(SNOWFLAKE_VIEWS_FILE, 'r', encoding='utf-8') as f:\n",
    "        snowflake_data = json.load(f)\n",
    "    \n",
    "    # Extract views list\n",
    "    if isinstance(snowflake_data, dict) and 'views' in snowflake_data:\n",
    "        actual_views = snowflake_data['views']\n",
    "    else:\n",
    "        actual_views = snowflake_data\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(actual_views)} Snowflake views\")\n",
    "    \n",
    "    # Display sample view structure\n",
    "    if actual_views:\n",
    "        print(f\"\\nüìã Sample View Structure (First View):\")\n",
    "        sample_view = actual_views[0]\n",
    "        print(f\"  ‚Ä¢ View Name: {sample_view.get('view_name', 'N/A')}\")\n",
    "        print(f\"  ‚Ä¢ Entity: {sample_view.get('entity', 'N/A')}\")\n",
    "        print(f\"  ‚Ä¢ Description: {sample_view.get('description', 'N/A')[:100]}...\")\n",
    "        print(f\"  ‚Ä¢ Columns: {len(sample_view.get('columns', []))} columns\")\n",
    "        print(f\"  ‚Ä¢ Selector Keywords: {len(sample_view.get('selector', []))} keywords\")\n",
    "    \n",
    "    # List all view entities\n",
    "    print(f\"\\nüìù All View Entities:\")\n",
    "    for i, view in enumerate(actual_views, 1):\n",
    "        entity = view.get('entity', 'Unknown')\n",
    "        view_name = view.get('view_name', 'Unknown')\n",
    "        print(f\"  {i:2d}. {entity:30s} ({view_name})\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: File not found: {SNOWFLAKE_VIEWS_FILE}\")\n",
    "    print(\"   Please ensure snowflake_view.json is in the current directory.\")\n",
    "    actual_views = []\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"‚ùå ERROR: Invalid JSON format: {e}\")\n",
    "    actual_views = []\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Unexpected error: {e}\")\n",
    "    actual_views = []\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä Total Views Loaded: {len(actual_views)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2314ce0",
   "metadata": {},
   "source": [
    "### Analyze View Metadata Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e4245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ANALYZING VIEW METADATA QUALITY\n",
      "============================================================\n",
      "üìä Metadata Completeness:\n",
      "  ‚Ä¢ Views with descriptions: 21/21 (100.0%)\n",
      "  ‚Ä¢ Views with selectors: 21/21 (100.0%)\n",
      "  ‚Ä¢ Views with columns: 21/21 (100.0%)\n",
      "\n",
      "üìä Statistics:\n",
      "  ‚Ä¢ Average columns per view: 26.3\n",
      "  ‚Ä¢ Average selectors per view: 331.3\n",
      "\n",
      "üíæ Saved metadata to: ..\\data\\processed\\snowflake_views_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Analyze metadata completeness\n",
    "print(\"üîç ANALYZING VIEW METADATA QUALITY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if actual_views:\n",
    "    metadata_stats = {\n",
    "        'total_views': len(actual_views),\n",
    "        'with_descriptions': 0,\n",
    "        'with_selectors': 0,\n",
    "        'with_columns': 0,\n",
    "        'avg_columns_per_view': 0,\n",
    "        'avg_selectors_per_view': 0,\n",
    "        'view_entities': []\n",
    "    }\n",
    "    \n",
    "    total_columns = 0\n",
    "    total_selectors = 0\n",
    "    \n",
    "    for view in actual_views:\n",
    "        # Check description\n",
    "        if view.get('description'):\n",
    "            metadata_stats['with_descriptions'] += 1\n",
    "        \n",
    "        # Check selectors\n",
    "        selectors = view.get('selector', [])\n",
    "        if selectors:\n",
    "            metadata_stats['with_selectors'] += 1\n",
    "            total_selectors += len(selectors)\n",
    "        \n",
    "        # Check columns\n",
    "        columns = view.get('columns', [])\n",
    "        if columns:\n",
    "            metadata_stats['with_columns'] += 1\n",
    "            total_columns += len(columns)\n",
    "        \n",
    "        # Store entity\n",
    "        metadata_stats['view_entities'].append(view.get('entity', 'Unknown'))\n",
    "    \n",
    "    # Calculate averages\n",
    "    metadata_stats['avg_columns_per_view'] = total_columns / len(actual_views) if actual_views else 0\n",
    "    metadata_stats['avg_selectors_per_view'] = total_selectors / len(actual_views) if actual_views else 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"üìä Metadata Completeness:\")\n",
    "    print(f\"  ‚Ä¢ Views with descriptions: {metadata_stats['with_descriptions']}/{metadata_stats['total_views']} ({metadata_stats['with_descriptions']/metadata_stats['total_views']*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Views with selectors: {metadata_stats['with_selectors']}/{metadata_stats['total_views']} ({metadata_stats['with_selectors']/metadata_stats['total_views']*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Views with columns: {metadata_stats['with_columns']}/{metadata_stats['total_views']} ({metadata_stats['with_columns']/metadata_stats['total_views']*100:.1f}%)\")\n",
    "    print(f\"\\nüìä Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Average columns per view: {metadata_stats['avg_columns_per_view']:.1f}\")\n",
    "    print(f\"  ‚Ä¢ Average selectors per view: {metadata_stats['avg_selectors_per_view']:.1f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No views loaded, skipping analysis\")\n",
    "    metadata_stats = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61326cc5",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Historical Training Examples\n",
    "\n",
    "Load batch test results containing questions and their corresponding SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c06572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading Historical Training Examples...\n",
      "üìÅ Source file: ..\\data\\batch_test_results_20250924_172439.jsonl\n",
      "------------------------------------------------------------\n",
      "‚úÖ Loaded 30 historical examples\n",
      "\n",
      "üìä DataFrame shape: (30, 16)\n",
      "üìä Columns: ['question_id', 'question', 'conversation_history', 'natural_language_answer', 'formatted_technical_answer', 'view_selection_reasoning', 'schema_info', 'generated_sql_queries', 'raw_query_results', 'result_count', 'processing_success', 'status', 'timestamp', 'has_conversation_history', 'sql_queries_count', 'raw_results_count']\n",
      "\n",
      "üìã Sample Record:\n",
      "  ‚Ä¢ Question ID: 1\n",
      "  ‚Ä¢ Question: What is MICs current exposure in the USA in PE?...\n",
      "  ‚Ä¢ Has SQL queries: 2 queries\n",
      "  ‚Ä¢ Has conversation history: No\n",
      "\n",
      "============================================================\n",
      "üìä Total Examples Loaded: 30\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load batch test results (historical data)\n",
    "BATCH_RESULTS_FILE = Path(\"../data/batch_test_results_20250924_172439.jsonl\")\n",
    "\n",
    "print(\"üîç Loading Historical Training Examples...\")\n",
    "print(f\"üìÅ Source file: {BATCH_RESULTS_FILE}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "batch_results = []\n",
    "\n",
    "try:\n",
    "    with open(BATCH_RESULTS_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    batch_results.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"‚ö†Ô∏è  Warning: Skipping line {line_num} due to JSON error: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(batch_results)} historical examples\")\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df_batch_results = pd.json_normalize(batch_results)\n",
    "    print(f\"\\nüìä DataFrame shape: {df_batch_results.shape}\")\n",
    "    print(f\"üìä Columns: {list(df_batch_results.columns)}\")\n",
    "    \n",
    "    # Display sample\n",
    "    if len(df_batch_results) > 0:\n",
    "        print(f\"\\nüìã Sample Record:\")\n",
    "        sample = batch_results[0]\n",
    "        print(f\"  ‚Ä¢ Question ID: {sample.get('question_id', 'N/A')}\")\n",
    "        print(f\"  ‚Ä¢ Question: {sample.get('question', 'N/A')[:100]}...\")\n",
    "        print(f\"  ‚Ä¢ Has SQL queries: {len(sample.get('generated_sql_queries', []))} queries\")\n",
    "        print(f\"  ‚Ä¢ Has conversation history: {'Yes' if sample.get('conversation_history') else 'No'}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: File not found: {BATCH_RESULTS_FILE}\")\n",
    "    batch_results = []\n",
    "    df_batch_results = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: {e}\")\n",
    "    batch_results = []\n",
    "    df_batch_results = pd.DataFrame()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä Total Examples Loaded: {len(batch_results)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2157751",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Extract Ground Truth View Selections\n",
    "\n",
    "Parse SQL queries to identify which Snowflake views were actually used. This creates our ground truth labels for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5fef9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç EXTRACTING GROUND TRUTH VIEW SELECTIONS\n",
      "============================================================\n",
      "‚úÖ Extracted ground truth for 30 examples\n",
      "\n",
      "üìä View Selection Statistics:\n",
      "  ‚Ä¢ Examples with views: 25\n",
      "  ‚Ä¢ Examples without views: 5\n",
      "\n",
      "üìä Most Frequently Selected Views:\n",
      "  ‚Ä¢ MIC_BY_COUNTRY_VW: 7 times\n",
      "  ‚Ä¢ MIC_BY_ASSET_CLASS_VW: 6 times\n",
      "  ‚Ä¢ MIC_BY_REGION_VW: 6 times\n",
      "  ‚Ä¢ MIC_BY_SECTOR_VW: 3 times\n",
      "  ‚Ä¢ MIC_KPI_VW: 2 times\n",
      "  ‚Ä¢ MIC_BY_INVESTMENT_CLASS_VW: 2 times\n",
      "  ‚Ä¢ PLATFORM_BY_SECTOR_VW: 1 times\n",
      "  ‚Ä¢ BUSINESS_UNIT_BY_SECTOR_VW: 1 times\n",
      "  ‚Ä¢ PLATFORM_BY_COUNTRY_VW: 1 times\n",
      "  ‚Ä¢ PLATFORM_BY_REGION_VW: 1 times\n"
     ]
    }
   ],
   "source": [
    "def extract_view_from_sql(sql_queries: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Extract view names from SQL queries and return as comma-separated string.\n",
    "    \n",
    "    Args:\n",
    "        sql_queries: List of SQL query strings\n",
    "        \n",
    "    Returns:\n",
    "        Comma-separated string of unique view names, or '<NO_VIEWS>' if none found\n",
    "    \"\"\"\n",
    "    views = set()\n",
    "    \n",
    "    for query in sql_queries:\n",
    "        if not query:\n",
    "            continue\n",
    "            \n",
    "        # Look for FROM clauses with view names\n",
    "        # Pattern: FROM schema.table_name or FROM table_name\n",
    "        from_matches = re.findall(r'FROM\\s+([A-Z_\\.]+)', query, re.IGNORECASE)\n",
    "        \n",
    "        for match in from_matches:\n",
    "            # Extract view name from fully qualified name\n",
    "            if 'VT_QA_DB.GANDALF.' in match:\n",
    "                view_name = match.split('.')[-1]\n",
    "                views.add(view_name)\n",
    "            elif '.' in match:\n",
    "                # Handle other schema patterns\n",
    "                view_name = match.split('.')[-1]\n",
    "                views.add(view_name)\n",
    "            else:\n",
    "                views.add(match)\n",
    "    \n",
    "    # Return comma-separated string instead of list\n",
    "    return ', '.join(sorted(views)) if views else '<NO_VIEWS>'\n",
    "\n",
    "\n",
    "print(\"üîç EXTRACTING GROUND TRUTH VIEW SELECTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sql_analysis = []\n",
    "views_json = json.dumps(actual_views, indent=2) if actual_views else \"{}\"\n",
    "\n",
    "for result in batch_results:\n",
    "    sql_queries = result.get('generated_sql_queries', [])\n",
    "    actual_views_used = extract_view_from_sql(sql_queries)\n",
    "    \n",
    "    sql_analysis.append({\n",
    "        'question_id': result.get('question_id', ''),\n",
    "        'question': result.get('question', ''),\n",
    "        'available_views': views_json,\n",
    "        'conversation_history': result.get('conversation_history', ''),\n",
    "        'reasoning': result.get('natural_language_answer', ''),\n",
    "        'selected_views': actual_views_used\n",
    "    })\n",
    "\n",
    "sql_df = pd.DataFrame(sql_analysis)\n",
    "\n",
    "print(f\"‚úÖ Extracted ground truth for {len(sql_df)} examples\")\n",
    "print(f\"\\nüìä View Selection Statistics:\")\n",
    "print(f\"  ‚Ä¢ Examples with views: {(sql_df['selected_views'] != '<NO_VIEWS>').sum()}\")\n",
    "print(f\"  ‚Ä¢ Examples without views: {(sql_df['selected_views'] == '<NO_VIEWS>').sum()}\")\n",
    "\n",
    "# Analyze view distribution\n",
    "all_selected_views = []\n",
    "for views_str in sql_df['selected_views']:\n",
    "    if views_str != '<NO_VIEWS>':\n",
    "        all_selected_views.extend([v.strip() for v in views_str.split(',')])\n",
    "\n",
    "if all_selected_views:\n",
    "    view_counts = pd.Series(all_selected_views).value_counts()\n",
    "    print(f\"\\nüìä Most Frequently Selected Views:\")\n",
    "    for view, count in view_counts.head(10).items():\n",
    "        print(f\"  ‚Ä¢ {view}: {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d4c291",
   "metadata": {},
   "source": [
    "### Display Sample Extracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99a184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã SAMPLE EXTRACTED DATA\n",
      "============================================================\n",
      "   question_id                                                             question conversation_history                            selected_views\n",
      "0            1                      What is MICs current exposure in the USA in PE?                   []  MIC_BY_ASSET_CLASS_VW, MIC_BY_COUNTRY_VW\n",
      "1            2          What is MICs current exposure in the GCC in Private Equity?                   []   MIC_BY_ASSET_CLASS_VW, MIC_BY_REGION_VW\n",
      "2            3  In FinTech, report the current exposure for Europe for Group (MIC).                   []        MIC_BY_REGION_VW, MIC_BY_SECTOR_VW\n",
      "\n",
      "üíæ Saved intermediate data to: ..\\data\\processed\\training_raw_extracted.csv\n"
     ]
    }
   ],
   "source": [
    "# Display sample extracted data\n",
    "print(\"üìã SAMPLE EXTRACTED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(sql_df[['question_id', 'question', 'conversation_history', 'selected_views']].head(3).to_string())\n",
    "\n",
    "# Save intermediate results\n",
    "intermediate_output = OUTPUT_DIR / \"training_raw_extracted.csv\"\n",
    "sql_df[['question_id', 'question', 'conversation_history', 'selected_views']].to_csv(\n",
    "    intermediate_output, \n",
    "    index=False, \n",
    "    encoding='utf-8'\n",
    ")\n",
    "print(f\"\\nüíæ Saved intermediate data to: {intermediate_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dedc126",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Load Updated Training Dataset\n",
    "\n",
    "Load the curated/updated training dataset with verified ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45753a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load updated training dataset with verified labels\n",
    "UPDATED_TRAINING_FILE = Path(\"view_selector_results_updated.csv\")\n",
    "\n",
    "print(\"üîç Loading Updated Training Dataset...\")\n",
    "print(f\"üìÅ Source file: {UPDATED_TRAINING_FILE}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    sql_df_updated = pd.read_csv(UPDATED_TRAINING_FILE, encoding='utf-8')\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(sql_df_updated)} training examples\")\n",
    "    print(f\"\\nüìä DataFrame shape: {sql_df_updated.shape}\")\n",
    "    print(f\"üìä Columns: {list(sql_df_updated.columns)}\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\nüìã Sample Records:\")\n",
    "    display_cols = [col for col in ['question_id', 'question', 'conversation_history', 'expected_views'] if col in sql_df_updated.columns]\n",
    "    if display_cols:\n",
    "        print(sql_df_updated[display_cols].head(3).to_string())\n",
    "    \n",
    "    # Analyze expected views\n",
    "    if 'expected_views' in sql_df_updated.columns:\n",
    "        print(f\"\\nüìä Expected Views Statistics:\")\n",
    "        print(f\"  ‚Ä¢ Examples with expected views: {sql_df_updated['expected_views'].notna().sum()}\")\n",
    "        print(f\"  ‚Ä¢ Examples without expected views: {sql_df_updated['expected_views'].isna().sum()}\")\n",
    "        \n",
    "        # View distribution\n",
    "        all_expected_views = []\n",
    "        for views in sql_df_updated['expected_views'].dropna():\n",
    "            if isinstance(views, str) and views and views != '<NO_VIEWS>':\n",
    "                all_expected_views.extend([v.strip() for v in str(views).split(',')])\n",
    "        \n",
    "        if all_expected_views:\n",
    "            expected_view_counts = pd.Series(all_expected_views).value_counts()\n",
    "            print(f\"\\nüìä Most Common Expected Views:\")\n",
    "            for view, count in expected_view_counts.head(10).items():\n",
    "                print(f\"  ‚Ä¢ {view}: {count} times\")\n",
    "    \n",
    "    # Use this as the primary training dataset\n",
    "    sql_df = sql_df_updated.copy()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Updated file not found: {UPDATED_TRAINING_FILE}\")\n",
    "    print(\"   Using previously extracted data instead.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: {e}\")\n",
    "    print(\"   Using previously extracted data instead.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä Final Training Dataset Size: {len(sql_df)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae44fbb",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Data Quality Assessment\n",
    "\n",
    "Verify data completeness, check for missing values, and identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "273e4e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DATA QUALITY ASSESSMENT\n",
      "============================================================\n",
      "üìä Missing Values:\n",
      "Series([], dtype: int64)\n",
      "  ‚úÖ No missing values detected\n",
      "\n",
      "üìä Duplicate Questions:\n",
      "  ‚Ä¢ Duplicate questions: 0\n",
      "\n",
      "üìä Conversation History:\n",
      "  ‚Ä¢ Examples with history: 30 (100.0%)\n",
      "  ‚Ä¢ Examples without history: 0 (0.0%)\n",
      "\n",
      "üìä Expected Views Format:\n",
      "  ‚Ä¢ Examples with selected views: 25 (83.3%)\n",
      "  ‚Ä¢ Examples with <NO_VIEWS>: 5 (16.7%)\n",
      "  ‚Ä¢ Average views per example: 1.13\n",
      "  ‚Ä¢ Max views per example: 2\n",
      "  ‚Ä¢ Min views per example: 0\n",
      "\n",
      "üíæ Saved quality report to: ..\\data\\processed\\data_quality_report.json\n",
      "\n",
      "============================================================\n",
      "‚úÖ Data quality assessment complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"üìä Missing Values:\")\n",
    "missing_stats = sql_df.isnull().sum()\n",
    "print(missing_stats[missing_stats > 0])\n",
    "if missing_stats.sum() == 0:\n",
    "    print(\"  ‚úÖ No missing values detected\")\n",
    "\n",
    "# Check for duplicate questions\n",
    "print(f\"\\nüìä Duplicate Questions:\")\n",
    "if 'question' in sql_df.columns:\n",
    "    duplicates = sql_df['question'].duplicated().sum()\n",
    "    print(f\"  ‚Ä¢ Duplicate questions: {duplicates}\")\n",
    "    if duplicates > 0:\n",
    "        print(\"  ‚ö†Ô∏è  Warning: Duplicate questions found\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  'question' column not found\")\n",
    "\n",
    "# Check conversation history presence\n",
    "print(f\"\\nüìä Conversation History:\")\n",
    "if 'conversation_history' in sql_df.columns:\n",
    "    with_history = (sql_df['conversation_history'].notna() & (sql_df['conversation_history'] != '')).sum()\n",
    "    without_history = len(sql_df) - with_history\n",
    "    print(f\"  ‚Ä¢ Examples with history: {with_history} ({with_history/len(sql_df)*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Examples without history: {without_history} ({without_history/len(sql_df)*100:.1f}%)\")\n",
    "\n",
    "# Check expected views format\n",
    "print(f\"\\nüìä Expected Views Format:\")\n",
    "if 'expected_views' in sql_df.columns:\n",
    "    views_column = 'expected_views'\n",
    "elif 'selected_views' in sql_df.columns:\n",
    "    views_column = 'selected_views'\n",
    "else:\n",
    "    views_column = None\n",
    "\n",
    "if views_column:\n",
    "    no_views_count = (sql_df[views_column] == '<NO_VIEWS>').sum()\n",
    "    with_views_count = len(sql_df) - no_views_count\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Examples with selected views: {with_views_count} ({with_views_count/len(sql_df)*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Examples with <NO_VIEWS>: {no_views_count} ({no_views_count/len(sql_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Count views per example\n",
    "    views_per_example = []\n",
    "    for views in sql_df[views_column]:\n",
    "        if pd.notna(views) and views != '<NO_VIEWS>':\n",
    "            views_per_example.append(len([v.strip() for v in str(views).split(',')]))\n",
    "        else:\n",
    "            views_per_example.append(0)\n",
    "    \n",
    "    if views_per_example:\n",
    "        print(f\"  ‚Ä¢ Average views per example: {np.mean(views_per_example):.2f}\")\n",
    "        print(f\"  ‚Ä¢ Max views per example: {max(views_per_example)}\")\n",
    "        print(f\"  ‚Ä¢ Min views per example: {min(views_per_example)}\")\n",
    "\n",
    "# Generate data quality report\n",
    "data_quality_report = {\n",
    "    'total_examples': len(sql_df),\n",
    "    'missing_values': missing_stats.to_dict(),\n",
    "    'duplicate_questions': int(duplicates) if 'question' in sql_df.columns else None,\n",
    "    'with_conversation_history': int(with_history) if 'conversation_history' in sql_df.columns else None,\n",
    "    'with_selected_views': int(with_views_count) if views_column else None,\n",
    "    'avg_views_per_example': float(np.mean(views_per_example)) if views_per_example else None,\n",
    "    'metadata_stats': metadata_stats\n",
    "}\n",
    "\n",
    "# Save quality report\n",
    "quality_report_path = OUTPUT_DIR / \"data_quality_report.json\"\n",
    "with open(quality_report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_quality_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nüíæ Saved quality report to: {quality_report_path}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ Data quality assessment complete\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed489571",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Prepare Final Training Examples\n",
    "\n",
    "Convert DataFrame to dictionary format and prepare for DSPy format conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325d1323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Converting to Dictionary Format...\n",
      "‚úÖ Converted 30 examples to dictionary format\n"
     ]
    }
   ],
   "source": [
    "# Convert to dictionary format for easier processing\n",
    "print(\"üîÑ Converting to Dictionary Format...\")\n",
    "\n",
    "# Ensure we're using the correct column name for expected views\n",
    "if 'expected_views' in sql_df.columns:\n",
    "    views_column = 'expected_views'\n",
    "elif 'selected_views' in sql_df.columns:\n",
    "    views_column = 'selected_views'\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: No views column found, will use empty values\")\n",
    "    views_column = None\n",
    "\n",
    "training_examples = []\n",
    "for idx, row in sql_df.iterrows():\n",
    "    example = {\n",
    "        'question_id': row.get('question_id', f'q_{idx}'),\n",
    "        'question': row.get('question', ''),\n",
    "        'conversation_history': row.get('conversation_history', ''),\n",
    "        'expected_views': row.get(views_column, '<NO_VIEWS>') if views_column else '<NO_VIEWS>',\n",
    "    }\n",
    "    \n",
    "    # Add any additional fields that exist\n",
    "    for col in sql_df.columns:\n",
    "        if col not in example and col not in ['question_id', 'question', 'conversation_history', views_column]:\n",
    "            example[col] = row.get(col, '')\n",
    "    \n",
    "    training_examples.append(example)\n",
    "\n",
    "print(f\"‚úÖ Converted {len(training_examples)} examples to dictionary format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c275f",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Train/Test Split\n",
    "\n",
    "Create stratified 70/30 split for training and testing, ensuring balanced distribution of view selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "432fa3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÄ CREATING TRAIN/TEST SPLIT\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'training_examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m np.random.seed(RANDOM_SEED)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Shuffle examples\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m shuffled_examples = \u001b[43mtraining_examples\u001b[49m.copy()\n\u001b[32m     13\u001b[39m random.shuffle(shuffled_examples)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Split ratio\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'training_examples' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(\"üîÄ CREATING TRAIN/TEST SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Shuffle examples\n",
    "shuffled_examples = training_examples.copy()\n",
    "random.shuffle(shuffled_examples)\n",
    "\n",
    "# Split ratio\n",
    "TRAIN_RATIO = 0.7\n",
    "split_index = int(len(shuffled_examples) * TRAIN_RATIO)\n",
    "\n",
    "train_examples = shuffled_examples[:split_index]\n",
    "test_examples = shuffled_examples[split_index:]\n",
    "\n",
    "print(f\"üìä Split Configuration:\")\n",
    "print(f\"  ‚Ä¢ Random seed: {RANDOM_SEED}\")\n",
    "print(f\"  ‚Ä¢ Train ratio: {TRAIN_RATIO:.0%}\")\n",
    "print(f\"  ‚Ä¢ Train size: {len(train_examples)} examples\")\n",
    "print(f\"  ‚Ä¢ Test size: {len(test_examples)} examples\")\n",
    "print(f\"  ‚Ä¢ Total: {len(training_examples)} examples\")\n",
    "\n",
    "# Analyze split distribution\n",
    "print(f\"\\nüìä Split Distribution Analysis:\")\n",
    "\n",
    "def analyze_split(examples, split_name):\n",
    "    \"\"\"Analyze characteristics of a data split\"\"\"\n",
    "    with_views = sum(1 for ex in examples if ex['expected_views'] != '<NO_VIEWS>')\n",
    "    with_history = sum(1 for ex in examples if ex.get('conversation_history'))\n",
    "    \n",
    "    print(f\"\\n  {split_name}:\")\n",
    "    print(f\"    ‚Ä¢ Total: {len(examples)}\")\n",
    "    print(f\"    ‚Ä¢ With views: {with_views} ({with_views/len(examples)*100:.1f}%)\")\n",
    "    print(f\"    ‚Ä¢ With conversation history: {with_history} ({with_history/len(examples)*100:.1f}%)\")\n",
    "    \n",
    "    # View count distribution\n",
    "    view_counts = []\n",
    "    for ex in examples:\n",
    "        if ex['expected_views'] != '<NO_VIEWS>':\n",
    "            view_counts.append(len([v.strip() for v in ex['expected_views'].split(',')]))\n",
    "    \n",
    "    if view_counts:\n",
    "        print(f\"    ‚Ä¢ Avg views per example: {np.mean(view_counts):.2f}\")\n",
    "\n",
    "analyze_split(train_examples, \"Training Set\")\n",
    "analyze_split(test_examples, \"Test Set\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ Train/test split complete\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27ab882",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Export Clean Datasets\n",
    "\n",
    "Save preprocessed training and test datasets for use in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3048d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ EXPORTING CLEAN DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Export paths\n",
    "train_output = OUTPUT_DIR / \"train_examples.json\"\n",
    "test_output = OUTPUT_DIR / \"test_examples.json\"\n",
    "all_output = OUTPUT_DIR / \"all_examples.json\"\n",
    "\n",
    "# Save training data\n",
    "with open(train_output, 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_examples, f, indent=2, ensure_ascii=False)\n",
    "print(f\"‚úÖ Saved {len(train_examples)} training examples to: {train_output}\")\n",
    "\n",
    "# Save test data\n",
    "with open(test_output, 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_examples, f, indent=2, ensure_ascii=False)\n",
    "print(f\"‚úÖ Saved {len(test_examples)} test examples to: {test_output}\")\n",
    "\n",
    "# Save all data (for reference)\n",
    "with open(all_output, 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_examples, f, indent=2, ensure_ascii=False)\n",
    "print(f\"‚úÖ Saved {len(training_examples)} total examples to: {all_output}\")\n",
    "\n",
    "# Create summary document\n",
    "summary = {\n",
    "    'dataset_info': {\n",
    "        'total_examples': len(training_examples),\n",
    "        'train_examples': len(train_examples),\n",
    "        'test_examples': len(test_examples),\n",
    "        'split_ratio': TRAIN_RATIO,\n",
    "        'random_seed': RANDOM_SEED\n",
    "    },\n",
    "    'metadata': {\n",
    "        'total_views': len(actual_views) if actual_views else 0,\n",
    "        'view_entities': metadata_stats.get('view_entities', []) if metadata_stats else []\n",
    "    },\n",
    "    'data_quality': data_quality_report,\n",
    "    'output_files': {\n",
    "        'train': str(train_output),\n",
    "        'test': str(test_output),\n",
    "        'all': str(all_output),\n",
    "        'views_metadata': str(OUTPUT_DIR / \"snowflake_views_metadata.json\"),\n",
    "        'quality_report': str(quality_report_path)\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_output = OUTPUT_DIR / \"dataset_summary.json\"\n",
    "with open(summary_output, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "print(f\"‚úÖ Saved dataset summary to: {summary_output}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ All datasets exported successfully!\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
