{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52feb667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity.aio import AzureCliCredential\n",
    "from agent_framework.azure import AzureAIClient\n",
    "from src.modules import ViewSelectorModule\n",
    "from src.data_loader import load_snowflake_views\n",
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5081b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configure DSPy and load data\n",
    "lm = dspy.LM(\"azure/gpt-4o\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5754ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 21 views directly\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/snowflake_view.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "    snowflake_views = data.get('views', [])\n",
    "    print(f\"âœ… Loaded {len(snowflake_views)} views directly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snowflake_views = load_snowflake_views(\"../data/snowflake_view.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f76be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules_v2 import IntegratedText2SQLModule\n",
    "view_selector = IntegratedText2SQLModule(config = {},candidate_views=snowflake_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2193ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a fresh module\n",
    "# view_selector = ViewSelectorModule(candidate_views=snowflake_views)\n",
    "# # Load the optimized state\n",
    "# view_selector.load(\"../data/optimized_modules/gepa_module.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6e3bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    entity_reasoning='1. **Identify terms in query that match entity candidates:**\\n   - The query mentions \"Private Equity\" and \"platform\".\\n   - \"Private Equity\" is a term that appears in both PLATFORM and ASSET_CLASS categories.\\n\\n2. **Apply disambiguation rules based on context keywords:**\\n   - The query includes the word \"platform\", which suggests that we should prefer the PLATFORM entity type over ASSET_CLASS.\\n\\n3. **Match terms to EXACT values from entity_mapper.json:**\\n   - \"Private Equity\" matches exactly with a value in the PLATFORM category: \"Private Equity\".\\n\\n4. **Validate: no partial matches, no JSON artifacts, proper capitalization:**\\n   - The matched term \"Private Equity\" is an exact match with the correct capitalization as listed in the PLATFORM category.\\n   - No partial matches or JSON artifacts are present in the resolved entities.',\n",
      "    resolved_entities='{\"PLATFORM\": \"Private Equity\"}',\n",
      "    view_reasoning='1. **Analyze question keywords and resolved entities**: The question asks for \"MIC\\'s current exposure in the GCC in Private Equity platform.\" The resolved entity is \"PLATFORM: Private Equity.\" The keyword \"MIC\\'s exposure\" indicates a portfolio-level query, not specific to a platform.\\n\\n2. **Match entities to appropriate view dimensions**: The question involves geographical dimensions (GCC) and platform-level aggregation (Private Equity). However, the mention of \"MIC\\'s exposure\" suggests a portfolio-level view is needed.\\n\\n3. **Consider special cases**: According to the domain knowledge, when the user asks about \"MIC\\'s exposure,\" it refers to portfolio-level aggregation, not a specific platform. Therefore, MIC_* views should be considered.\\n\\n4. **Evaluate each candidate view\\'s relevance**:\\n   - **PLATFORM_KPI_VW**: Aggregates at the platform level only, not suitable for portfolio-level queries.\\n   - **MIC_BY_REGION_VW**: Aggregates at the region level for all of Mubadala\\'s investments, suitable for portfolio-level queries but lacks platform specificity.\\n   - **PLATFORM_BY_REGION_VW**: Aggregates at both region and platform level, which matches the need for geographical and platform-specific data.\\n\\n5. **Final decision with justification**: The question requires data aggregated by both region (GCC) and platform (Private Equity). The most suitable view is **PLATFORM_BY_REGION_VW**, as it provides the necessary breakdown by both dimensions.',\n",
      "    selected_views='PLATFORM_BY_REGION_VW',\n",
      "    combined_reasoning='=== STAGE 1: ENTITY RESOLUTION ===\\n1. **Identify terms in query that match entity candidates:**\\n   - The query mentions \"Private Equity\" and \"platform\".\\n   - \"Private Equity\" is a term that appears in both PLATFORM and ASSET_CLASS categories.\\n\\n2. **Apply disambiguation rules based on context keywords:**\\n   - The query includes the word \"platform\", which suggests that we should prefer the PLATFORM entity type over ASSET_CLASS.\\n\\n3. **Match terms to EXACT values from entity_mapper.json:**\\n   - \"Private Equity\" matches exactly with a value in the PLATFORM category: \"Private Equity\".\\n\\n4. **Validate: no partial matches, no JSON artifacts, proper capitalization:**\\n   - The matched term \"Private Equity\" is an exact match with the correct capitalization as listed in the PLATFORM category.\\n   - No partial matches or JSON artifacts are present in the resolved entities.\\n\\nResolved Entities: {\"PLATFORM\": \"Private Equity\"}\\n\\n=== STAGE 2: VIEW SELECTION ===\\n1. **Analyze question keywords and resolved entities**: The question asks for \"MIC\\'s current exposure in the GCC in Private Equity platform.\" The resolved entity is \"PLATFORM: Private Equity.\" The keyword \"MIC\\'s exposure\" indicates a portfolio-level query, not specific to a platform.\\n\\n2. **Match entities to appropriate view dimensions**: The question involves geographical dimensions (GCC) and platform-level aggregation (Private Equity). However, the mention of \"MIC\\'s exposure\" suggests a portfolio-level view is needed.\\n\\n3. **Consider special cases**: According to the domain knowledge, when the user asks about \"MIC\\'s exposure,\" it refers to portfolio-level aggregation, not a specific platform. Therefore, MIC_* views should be considered.\\n\\n4. **Evaluate each candidate view\\'s relevance**:\\n   - **PLATFORM_KPI_VW**: Aggregates at the platform level only, not suitable for portfolio-level queries.\\n   - **MIC_BY_REGION_VW**: Aggregates at the region level for all of Mubadala\\'s investments, suitable for portfolio-level queries but lacks platform specificity.\\n   - **PLATFORM_BY_REGION_VW**: Aggregates at both region and platform level, which matches the need for geographical and platform-specific data.\\n\\n5. **Final decision with justification**: The question requires data aggregated by both region (GCC) and platform (Private Equity). The most suitable view is **PLATFORM_BY_REGION_VW**, as it provides the necessary breakdown by both dimensions.\\n\\nSelected Views: PLATFORM_BY_REGION_VW',\n",
      "    reasoning='=== STAGE 1: ENTITY RESOLUTION ===\\n1. **Identify terms in query that match entity candidates:**\\n   - The query mentions \"Private Equity\" and \"platform\".\\n   - \"Private Equity\" is a term that appears in both PLATFORM and ASSET_CLASS categories.\\n\\n2. **Apply disambiguation rules based on context keywords:**\\n   - The query includes the word \"platform\", which suggests that we should prefer the PLATFORM entity type over ASSET_CLASS.\\n\\n3. **Match terms to EXACT values from entity_mapper.json:**\\n   - \"Private Equity\" matches exactly with a value in the PLATFORM category: \"Private Equity\".\\n\\n4. **Validate: no partial matches, no JSON artifacts, proper capitalization:**\\n   - The matched term \"Private Equity\" is an exact match with the correct capitalization as listed in the PLATFORM category.\\n   - No partial matches or JSON artifacts are present in the resolved entities.\\n\\nResolved Entities: {\"PLATFORM\": \"Private Equity\"}\\n\\n=== STAGE 2: VIEW SELECTION ===\\n1. **Analyze question keywords and resolved entities**: The question asks for \"MIC\\'s current exposure in the GCC in Private Equity platform.\" The resolved entity is \"PLATFORM: Private Equity.\" The keyword \"MIC\\'s exposure\" indicates a portfolio-level query, not specific to a platform.\\n\\n2. **Match entities to appropriate view dimensions**: The question involves geographical dimensions (GCC) and platform-level aggregation (Private Equity). However, the mention of \"MIC\\'s exposure\" suggests a portfolio-level view is needed.\\n\\n3. **Consider special cases**: According to the domain knowledge, when the user asks about \"MIC\\'s exposure,\" it refers to portfolio-level aggregation, not a specific platform. Therefore, MIC_* views should be considered.\\n\\n4. **Evaluate each candidate view\\'s relevance**:\\n   - **PLATFORM_KPI_VW**: Aggregates at the platform level only, not suitable for portfolio-level queries.\\n   - **MIC_BY_REGION_VW**: Aggregates at the region level for all of Mubadala\\'s investments, suitable for portfolio-level queries but lacks platform specificity.\\n   - **PLATFORM_BY_REGION_VW**: Aggregates at both region and platform level, which matches the need for geographical and platform-specific data.\\n\\n5. **Final decision with justification**: The question requires data aggregated by both region (GCC) and platform (Private Equity). The most suitable view is **PLATFORM_BY_REGION_VW**, as it provides the necessary breakdown by both dimensions.\\n\\nSelected Views: PLATFORM_BY_REGION_VW'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "prediction = view_selector(\n",
    "        question=\"What is MICs current exposure in the GCC in Private Equity platform?\",\n",
    "        conversation_history=[]\n",
    "        )\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e186d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_views(question: str, conversation_history: str = \"\") -> dict:\n",
    "    \"\"\"\n",
    "    Select relevant database views for a given question.\n",
    "    \n",
    "    Args:\n",
    "        question: User's natural language database query\n",
    "        conversation_history: Previous conversation context (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with selected views and reasoning\n",
    "    \"\"\"\n",
    "    prediction = view_selector(\n",
    "        question=question,\n",
    "        conversation_history=conversation_history\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"selected_views\": prediction.selected_views,\n",
    "        \"reasoning\": prediction.reasoning\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e8e9dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tool functions created: select_relevant_views, get_entity_schema\n"
     ]
    }
   ],
   "source": [
    "def get_entity_schema(entity_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the schema information for a specific database view entity.\n",
    "    \n",
    "    Args:\n",
    "        entity_name: The entity name (e.g., 'INVESTMENT_KPI_VW', 'MIC_BY_COUNTRY_VW')\n",
    "    \n",
    "    Returns:\n",
    "        String with schema details including columns, types, and definitions\n",
    "    \"\"\"\n",
    "    # Find the view by entity name\n",
    "    view_info = None\n",
    "    view_display_name = None\n",
    "    \n",
    "    for view in snowflake_views:\n",
    "        if view.get('entity', '') == entity_name:\n",
    "            view_info = view\n",
    "            view_display_name = view.get('view_name', entity_name)\n",
    "            break\n",
    "    \n",
    "    if not view_info:\n",
    "        # List available entities for debugging\n",
    "        available_entities = [v.get('entity', '') for v in snowflake_views if v.get('entity')][:10]\n",
    "        return f\"âŒ Entity '{entity_name}' not found.\\n\\nAvailable entities: {', '.join(available_entities)}...\"\n",
    "    \n",
    "    # Build schema information\n",
    "    schema_info = f\"\"\"**Schema for {view_display_name}**\n",
    "**Entity:** {entity_name}\n",
    "**Description:** {view_info.get('description', 'N/A')}\n",
    "\n",
    "**Columns:**\n",
    "\"\"\"\n",
    "    \n",
    "    if 'columns' in view_info and view_info['columns']:\n",
    "        for col in view_info['columns'][:20]:  # Limit to first 20 columns\n",
    "            col_name = col.get('name', 'Unknown')\n",
    "            col_type = col.get('type', 'Unknown')\n",
    "            col_def = col.get('definition', '')\n",
    "            \n",
    "            schema_info += f\"\\nâ€¢ **{col_name}** ({col_type})\"\n",
    "            if col_def:\n",
    "                schema_info += f\"\\n  {col_def}\"\n",
    "            \n",
    "            # Add allowed values if present\n",
    "            if col.get('allowed_values'):\n",
    "                values = col['allowed_values'][:5]  # First 5 values\n",
    "                schema_info += f\"\\n  Allowed values: {', '.join(map(str, values))}\"\n",
    "                if len(col['allowed_values']) > 5:\n",
    "                    schema_info += f\" (+ {len(col['allowed_values']) - 5} more)\"\n",
    "            \n",
    "            schema_info += \"\\n\"\n",
    "        \n",
    "        if len(view_info['columns']) > 20:\n",
    "            schema_info += f\"\\n... and {len(view_info['columns']) - 20} more columns\"\n",
    "    else:\n",
    "        schema_info += \"No column information available\"\n",
    "    \n",
    "    return schema_info\n",
    "\n",
    "\n",
    "print(\"âœ… Tool functions created: select_relevant_views, get_entity_schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e334a9b6",
   "metadata": {},
   "source": [
    "### Example of running by MAF with 2 steps: \n",
    "- Select correct views\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab149a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def non_streaming_example() -> None:\n",
    "    \"\"\"Example of non-streaming response (get the complete result at once).\"\"\"\n",
    "    print(\"=== Non-streaming Response Example ===\")\n",
    "\n",
    "    # Since no Agent ID is provided, the agent will be automatically created.\n",
    "    # For authentication, run `az login` command in terminal or replace AzureCliCredential with preferred\n",
    "    # authentication option.\n",
    "    async with (\n",
    "        AzureCliCredential() as credential,\n",
    "        AzureAIClient(credential=credential).create_agent(\n",
    "            name=\"BasicWeatherAgent\",\n",
    "            instructions='''You are an expert Text-to-SQL agent for Snowflake databases.\n",
    "                Your workflow:\n",
    "                1. **Understand the question**: Identify what data the user needs\n",
    "                2. **Select views**: Use select_relevant_views to find which database views contain the needed data\n",
    "                3. **Get schema**: Use get_entity_schema for each selected view to understand the exact column names and types\n",
    "                4. **Generate SQL**: Write a Snowflake SQL query using the schema information\n",
    "\n",
    "                Important rules:\n",
    "                - Always call select_relevant_views first to identify relevant views. Display the output of select_relevant_views.\n",
    "                - Then call get_entity_schema for each selected view to see available columns\n",
    "                - Use exact column names from the schema (case-sensitive)\n",
    "                - Generate valid Snowflake SQL syntax\n",
    "                - Explain your reasoning at each step''',\n",
    "            tools=[select_relevant_views, get_entity_schema],\n",
    "        ) as agent,\n",
    "    ):\n",
    "        query = \"What is MICs current exposure in the GCC in Private Equity platform?\"\n",
    "        print(f\"User: {query}\")\n",
    "        async for chunk in agent.run_stream(query):\n",
    "            if chunk.text:\n",
    "                print(chunk.text, end=\"\", flush=True)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5159c61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Non-streaming Response Example ===\n",
      "User: What is MICs current exposure in the GCC in Private Equity platform?\n",
      "Based on the query and schema details, we need to identify MIC's current exposure in the GCC for the Private Equity platform. The relevant columns from the `PLATFORM_BY_REGION_VW` view are:\n",
      "\n",
      "1. **PLATFORM**: This will filter for \"Private Equity.\"\n",
      "2. **REGION**: This will filter for GCC countries under \"Middle East & North Africa.\"\n",
      "3. **UNREALIZED_VALUE**: Represents the exposure or fair market value.\n",
      "\n",
      "We will also retrieve the most recent data available, so we'll use the **AS_ON_DATE_ID** column to filter for the latest date.\n",
      "\n",
      "### Query\n",
      "Here is the SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT \n",
      "    AS_ON_DATE_ID,\n",
      "    PLATFORM,\n",
      "    REGION,\n",
      "    UNREALIZED_VALUE\n",
      "FROM \n",
      "    PLATFORM_BY_REGION_VW\n",
      "WHERE \n",
      "    PLATFORM = 'Private Equity' \n",
      "    AND REGION = 'Middle East & North Africa'\n",
      "    AND AS_ON_DATE_ID = (\n",
      "        SELECT MAX(AS_ON_DATE_ID)\n",
      "        FROM PLATFORM_BY_REGION_VW\n",
      "        WHERE PLATFORM = 'Private Equity' AND REGION = 'Middle East & North Africa'\n",
      "    );\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "1. **PLATFORM='Private Equity'**: Filters for the Private Equity platform.\n",
      "2. **REGION='Middle East & North Africa'**: Narrows down to GCC investments.\n",
      "3. **AS_ON_DATE_ID = (SELECT MAX(...))**: Ensures fetching the most current exposure data.\n",
      "\n",
      "Let me know if you'd like further analysis or modifications!\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "result = await non_streaming_example()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "439b279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Non-streaming Response Example ===\n",
      "User: What is MICs current exposure in the GCC in Private Equity platform?\n",
      "### Analysis of Query and Schema\n",
      "\n",
      "1. **Question Breakdown**:\n",
      "   - The user is asking for MIC's current exposure.\n",
      "   - \"GCC\" is a geographical location, and private equity is the investment platform.\n",
      "   - \"Current exposure\" aligns closely with the `UNREALIZED_VALUE`, which represents fair market value or exposure of the investment at a specific point in time.\n",
      "\n",
      "2. **Schema Review**:\n",
      "   - We need data aggregated by `PLATFORM` (Private Equity) and `REGION` (GCC). The schema of `PLATFORM_BY_REGION_VW` contains both these fields.\n",
      "   - `AS_ON_DATE_ID` will be used to get the most recent exposure data (the latest available date).\n",
      "   - Only rows where `PLATFORM = 'Private Equity'` and `REGION = 'Middle East & North Africa'` (to account for GCC) are relevant.\n",
      "\n",
      "### SQL Query\n",
      "Below is the SQL query that retrieves the current exposure for MIC in the GCC region under the Private Equity platform:\n",
      "\n",
      "```sql\n",
      "WITH Latest_Data AS (\n",
      "    SELECT \n",
      "        MAX(AS_ON_DATE_ID) AS Latest_Date\n",
      "    FROM \n",
      "        PLATFORM_BY_REGION_VW\n",
      ")\n",
      "SELECT \n",
      "    PLATFORM,\n",
      "    REGION,\n",
      "    UNREALIZED_VALUE AS Current_Exposure,\n",
      "    AS_ON_DATE_ID\n",
      "FROM \n",
      "    PLATFORM_BY_REGION_VW\n",
      "WHERE \n",
      "    PLATFORM = 'Private Equity'\n",
      "    AND REGION = 'Middle East & North Africa'\n",
      "    AND AS_ON_DATE_ID = (SELECT Latest_Date FROM Latest_Data);\n",
      "```\n",
      "\n",
      "### Explanation of Query\n",
      "1. **Latest_Data CTE**:\n",
      "   - This step fetches the most recent available date from the `AS_ON_DATE_ID` column.\n",
      "  \n",
      "2. **Main Query**:\n",
      "   - Filters the records where:\n",
      "     - `PLATFORM` is `\"Private Equity\"` to focus on the Private Equity platform.\n",
      "     - `REGION` is `\"Middle East & North Africa\"` to represent the GCC region.\n",
      "     - The date matches the latest available one from the `Latest_Data` CTE.\n",
      "   - Retrieves:\n",
      "     - `PLATFORM` and `REGION` for context.\n",
      "     - `UNREALIZED_VALUE` as the current exposure metric.\n",
      "     - `AS_ON_DATE_ID` to confirm the as-on date of the exposure.\n",
      "\n",
      "Let me know if you need this executed or further assistance!\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "result = await non_streaming_example()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48504e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7705b903",
   "metadata": {},
   "source": [
    "## Extended Architecture: Multi-Task DSPy + Agent Framework\n",
    "\n",
    "This notebook demonstrates how to integrate multiple specialized tasks:\n",
    "1. **View Selection** - IntegratedText2SQLModule (Entity Resolution + View Selection)\n",
    "2. **Schema Retrieval** - Get detailed column information\n",
    "3. **SQL Generation** - Generate optimized Snowflake queries\n",
    "4. **Answer Generation** - Interpret results and provide final answer\n",
    "\n",
    "**Architecture Pattern:**\n",
    "- **Agent Framework**: Orchestrates the workflow, manages conversation, coordinates tools\n",
    "- **DSPy Modules**: Handle heavy reasoning for each specialized task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TASK 1: View Selection with Entity Resolution (Already implemented above)\n",
    "# ============================================================================\n",
    "\n",
    "def select_views_with_entities(question: str, conversation_history: str = \"\") -> dict:\n",
    "    \"\"\"\n",
    "    Intelligent view selection using two-stage DSPy reasoning.\n",
    "    \n",
    "    This is the heavy reasoning task that:\n",
    "    1. Resolves business entities (e.g., 'PE' â†’ 'Private Equity')\n",
    "    2. Selects optimal database views based on resolved entities\n",
    "    \n",
    "    Returns complete reasoning trace for transparency.\n",
    "    \"\"\"\n",
    "    prediction = view_selector(\n",
    "        question=question,\n",
    "        conversation_history=conversation_history\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"resolved_entities\": prediction.resolved_entities,\n",
    "        \"selected_views\": prediction.selected_views,\n",
    "        \"entity_reasoning\": prediction.entity_reasoning,\n",
    "        \"view_reasoning\": prediction.view_reasoning,\n",
    "        \"combined_reasoning\": prediction.combined_reasoning\n",
    "    }\n",
    "\n",
    "print(\"âœ… Task 1: View Selection (with Entity Resolution)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5da004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TASK 2: Schema Retrieval (Already implemented above as get_entity_schema)\n",
    "# ============================================================================\n",
    "\n",
    "# Already defined above - this retrieves detailed column information\n",
    "# for selected views\n",
    "\n",
    "print(\"âœ… Task 2: Schema Retrieval (get_entity_schema)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e38031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TASK 3: SQL Query Generation (DSPy Module for Heavy Reasoning)\n",
    "# ============================================================================\n",
    "\n",
    "# Define SQL Writer Signature\n",
    "class SQLWriterSignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Generate optimized Snowflake SQL queries based on question, schema, and resolved entities.\n",
    "    \n",
    "    You are a Snowflake SQL expert. Generate syntactically correct, optimized SQL queries.\n",
    "    \"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(desc=\"User's natural language question\")\n",
    "    view_schemas: str = dspy.InputField(desc=\"Detailed schema information for selected views\")\n",
    "    resolved_entities: str = dspy.InputField(desc=\"Resolved business entities (JSON format)\")\n",
    "    \n",
    "    reasoning: str = dspy.OutputField(desc=\"Step-by-step SQL generation reasoning\")\n",
    "    sql_query: str = dspy.OutputField(desc=\"Complete Snowflake SQL query\")\n",
    "\n",
    "\n",
    "class SQLWriterModule(dspy.Module):\n",
    "    \"\"\"DSPy module for SQL query generation with Chain-of-Thought reasoning.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sql_writer_cot = dspy.ChainOfThought(SQLWriterSignature)\n",
    "    \n",
    "    def forward(self, question: str, view_schemas: str, resolved_entities: str):\n",
    "        \"\"\"Generate SQL query with reasoning.\"\"\"\n",
    "        result = self.sql_writer_cot(\n",
    "            question=question,\n",
    "            view_schemas=view_schemas,\n",
    "            resolved_entities=resolved_entities\n",
    "        )\n",
    "        return result\n",
    "\n",
    "\n",
    "# Initialize SQL Writer\n",
    "sql_writer_module = SQLWriterModule()\n",
    "\n",
    "\n",
    "def generate_sql_query(question: str, view_schemas: str, resolved_entities: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generate Snowflake SQL query using DSPy Chain-of-Thought reasoning.\n",
    "    \n",
    "    Args:\n",
    "        question: User's natural language question\n",
    "        view_schemas: Detailed schema information from get_entity_schema\n",
    "        resolved_entities: Business entities in JSON format\n",
    "    \n",
    "    Returns:\n",
    "        Dict with SQL query and reasoning\n",
    "    \"\"\"\n",
    "    prediction = sql_writer_module(\n",
    "        question=question,\n",
    "        view_schemas=view_schemas,\n",
    "        resolved_entities=resolved_entities\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"sql_query\": prediction.sql_query,\n",
    "        \"reasoning\": prediction.reasoning\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"âœ… Task 3: SQL Query Generation (DSPy Module)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TASK 4: Answer Generation (Lightweight Agent - Fast Response)\n",
    "# ============================================================================\n",
    "\n",
    "def generate_final_answer(question: str, sql_query: str, query_results: str, resolved_entities: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate business-friendly final answer from query results.\n",
    "    \n",
    "    This uses the Agent Framework's native capabilities (fast, no DSPy overhead).\n",
    "    For complex interpretations requiring heavy reasoning, consider using DSPy.\n",
    "    \n",
    "    Args:\n",
    "        question: Original user question\n",
    "        sql_query: SQL query that was executed\n",
    "        query_results: Results from query execution\n",
    "        resolved_entities: Business entities (JSON format)\n",
    "    \n",
    "    Returns:\n",
    "        String with final answer formatted for the agent to present\n",
    "    \"\"\"\n",
    "    # Return a structured prompt for the agent to interpret\n",
    "    # Agent Framework will handle the actual generation\n",
    "    context = f\"\"\"\n",
    "**Question:** {question}\n",
    "\n",
    "**Resolved Business Entities:**\n",
    "{resolved_entities}\n",
    "\n",
    "**SQL Query Executed:**\n",
    "```sql\n",
    "{sql_query}\n",
    "```\n",
    "\n",
    "**Query Results:**\n",
    "{query_results}\n",
    "\n",
    "**Your Task:**\n",
    "Based on the query results above, provide a clear, business-friendly answer to the user's question.\n",
    "Include:\n",
    "1. Direct answer to the question with specific numbers\n",
    "2. Context about what the data represents\n",
    "3. Any relevant insights or caveats\n",
    "\n",
    "Be concise and professional.\n",
    "\"\"\"\n",
    "    return context\n",
    "\n",
    "\n",
    "print(\"âœ… Task 4: Answer Generation (Lightweight Agent)\")\n",
    "print(\"   Using Agent Framework for fast response\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb02dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TASK 5: SQL Query Execution (Simple utility - not DSPy)\n",
    "# ============================================================================\n",
    "\n",
    "def execute_sql_query(sql_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute SQL query against Snowflake database.\n",
    "    \n",
    "    Note: This is a placeholder. In production, replace with actual Snowflake connection.\n",
    "    \n",
    "    Args:\n",
    "        sql_query: Snowflake SQL query to execute\n",
    "    \n",
    "    Returns:\n",
    "        Query results as formatted string (JSON or table)\n",
    "    \"\"\"\n",
    "    # Placeholder implementation\n",
    "    # In production, use snowflake.connector or similar\n",
    "    \n",
    "    # For demo purposes, return mock results\n",
    "    mock_results = \"\"\"\n",
    "    Query Results:\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ COUNTRY             â”‚ UNREALIZED_VALUE  â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚ USA                 â”‚ $2,450,000,000    â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \n",
    "    (1 row returned)\n",
    "    \"\"\"\n",
    "    \n",
    "    return mock_results\n",
    "\n",
    "\n",
    "print(\"âœ… Task 5: SQL Query Execution (Utility Function)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0e1e9f",
   "metadata": {},
   "source": [
    "## Complete Multi-Task Workflow with Agent Framework\n",
    "\n",
    "Now let's integrate all tasks into a complete workflow orchestrated by Microsoft Agent Framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08cfc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def complete_text2sql_workflow() -> None:\n",
    "    \"\"\"\n",
    "    Complete Text-to-SQL workflow orchestrated by Microsoft Agent Framework.\n",
    "    \n",
    "    Architecture:\n",
    "    - Agent Framework: Orchestrates the multi-step workflow + handles answer generation\n",
    "    - DSPy Modules: Handle heavy reasoning for critical tasks (View Selection, SQL Generation)\n",
    "    \n",
    "    Workflow Steps:\n",
    "    1. View Selection (IntegratedText2SQLModule - DSPy Heavy Reasoning)\n",
    "    2. Schema Retrieval (get_entity_schema - Lightweight)\n",
    "    3. SQL Generation (SQLWriterModule - DSPy Heavy Reasoning)\n",
    "    4. Query Execution (execute_sql_query - Lightweight)\n",
    "    5. Answer Generation (Agent Framework - Fast)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸš€ Complete Text-to-SQL Workflow with Agent Framework + DSPy\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    async with (\n",
    "        AzureCliCredential() as credential,\n",
    "        AzureAIClient(credential=credential).create_agent(\n",
    "            name=\"CompleteText2SQLAgent\",\n",
    "            instructions='''You are an expert end-to-end Text-to-SQL agent for Snowflake databases.\n",
    "\n",
    "Your complete workflow:\n",
    "\n",
    "**STEP 1: View Selection & Entity Resolution**\n",
    "- Call select_views_with_entities(question) to:\n",
    "  - Resolve business entities (e.g., 'PE' â†’ 'Private Equity', 'GCC' â†’ Gulf countries)\n",
    "  - Select relevant database views using Chain-of-Thought reasoning\n",
    "- Display the resolved entities and selected views\n",
    "\n",
    "**STEP 2: Schema Retrieval** (Fast Lookup)\n",
    "- For each selected view, call get_entity_schema(view_name) to get:\n",
    "  - Column names and types\n",
    "  - Column definitions\n",
    "  - Allowed values for categorical fields\n",
    "- Show the key columns that will be used\n",
    "\n",
    "**STEP 3: SQL Generation** \n",
    "- Call generate_sql_query(question, view_schemas, resolved_entities) to:\n",
    "  - Generate optimized Snowflake SQL using DSPy Chain-of-Thought\n",
    "  - Apply proper filters based on resolved entities\n",
    "  - Use exact column names from schema\n",
    "- Display the generated SQL query\n",
    "\n",
    "**STEP 4: Query Execution** (Database Call)\n",
    "- Call execute_sql_query(sql_query) to run the query\n",
    "- Show the results in a clear format\n",
    "\n",
    "**STEP 5: Answer Generation** (You handle this directly - Fast Response)\n",
    "- Call generate_final_answer(question, sql_query, results, entities) to get context\n",
    "- Based on the returned context and results, provide a clear business-friendly answer\n",
    "- Include:\n",
    "  * Direct answer with specific numbers\n",
    "  * Context about what the data represents\n",
    "  * Any relevant insights or observations\n",
    "- Keep it concise and professional\n",
    "\n",
    "**Important:**\n",
    "- Show your reasoning at each step\n",
    "- Display intermediate outputs for transparency\n",
    "- If any step fails, explain why and suggest next steps\n",
    "- Use DSPy tools (Steps 1 & 3) for complex reasoning where accuracy is critical\n",
    "- Handle final answer generation yourself for faster response''',\n",
    "            tools=[\n",
    "                select_views_with_entities,\n",
    "                get_entity_schema,\n",
    "                generate_sql_query,\n",
    "                execute_sql_query,\n",
    "                generate_final_answer\n",
    "            ],\n",
    "        ) as agent,\n",
    "    ):\n",
    "        query = \"What is MIC's current exposure in the USA in Private Equity?\"\n",
    "        print(f\"\\nğŸ‘¤ User Question: {query}\")\n",
    "        print(\"\\nğŸ¤– Agent Working...\\n\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        async for chunk in agent.run_stream(query):\n",
    "            if chunk.text:\n",
    "                print(chunk.text, end=\"\", flush=True)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"âœ… Workflow Complete!\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "print(\"âœ… Complete workflow function defined\")\n",
    "print(\"\\nRun: await complete_text2sql_workflow()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2560825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete workflow\n",
    "await complete_text2sql_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd504bd",
   "metadata": {},
   "source": [
    "## Architecture Summary\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         Microsoft Agent Framework (Orchestration Layer)             â”‚\n",
    "â”‚  â€¢ Multi-step workflow coordination                                 â”‚\n",
    "â”‚  â€¢ Tool invocation sequencing                                       â”‚\n",
    "â”‚  â€¢ Conversation management                                          â”‚\n",
    "â”‚  â€¢ Streaming responses                                              â”‚\n",
    "â”‚  â€¢ Final answer generation (FAST - no DSPy overhead)                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â”‚ Orchestrates 5 Tasks\n",
    "                         â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚                                  â”‚\n",
    "        â–¼                                  â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  DSPy Modules    â”‚            â”‚  Utility Functions   â”‚\n",
    "â”‚ (Heavy Reasoning)â”‚            â”‚  (Lightweight)       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â”‚                                  â”‚\n",
    "        â”‚                                  â”‚\n",
    "        â”œâ”€ Task 1: IntegratedText2SQL     â”œâ”€ Task 2: get_entity_schema\n",
    "        â”‚   â€¢ Entity Resolution (CoT)     â”‚   â€¢ Schema retrieval\n",
    "        â”‚   â€¢ View Selection (CoT)        â”‚   â€¢ Fast lookup\n",
    "        â”‚   â€¢ Time: ~3s                   â”‚   â€¢ Time: <0.1s\n",
    "        â”‚   â€¢ Critical accuracy           â”‚\n",
    "        â”‚                                 â”œâ”€ Task 4: execute_sql_query\n",
    "        â”œâ”€ Task 3: SQL Generation         â”‚   â€¢ Query execution\n",
    "        â”‚   â€¢ SQLWriterModule (CoT)       â”‚   â€¢ Database call\n",
    "        â”‚   â€¢ Time: ~2-3s                 â”‚   â€¢ Time: 1-10s\n",
    "        â”‚   â€¢ Critical accuracy           â”‚\n",
    "        â”‚                                 â””â”€ Task 5: generate_final_answer\n",
    "        â”‚                                     â€¢ Context preparation\n",
    "        â”‚                                     â€¢ Agent interprets (FAST!)\n",
    "        â”‚                                     â€¢ Time: ~1s\n",
    "```\n",
    "\n",
    "### Optimized Performance Profile:\n",
    "\n",
    "| Task | Component | Reasoning Complexity | Time | Optimizable |\n",
    "|------|-----------|---------------------|------|-------------|\n",
    "| 1. View Selection | DSPy | **High** - Entity resolution + multi-dimensional matching | ~3s | âœ… GEPA/Bootstrap |\n",
    "| 2. Schema Retrieval | Utility | Low - Direct lookup | <0.1s | âŒ Already fast |\n",
    "| 3. SQL Generation | DSPy | **High** - Schema mapping + filter logic | ~2-3s | âœ… Bootstrap |\n",
    "| 4. Query Execution | Utility | None - Database call | 1-10s | âŒ DB performance |\n",
    "| 5. Answer Generation | **Agent** | Low - Interpretation | ~1s | âŒ Fast enough |\n",
    "\n",
    "**Total Time: ~7-17 seconds** (acceptable for production)\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "1. **Optimized Performance** âš¡\n",
    "   - DSPy only for critical reasoning tasks (Steps 1 & 3)\n",
    "   - Agent handles simple interpretation (Step 5) - Fast!\n",
    "   - 2-3 seconds saved on final answer generation\n",
    "\n",
    "2. **Strategic Reasoning Allocation** ğŸ¯\n",
    "   - **Heavy DSPy reasoning**: Where accuracy is critical (View Selection, SQL Generation)\n",
    "   - **Lightweight Agent**: Where speed matters more (Answer formatting)\n",
    "\n",
    "3. **Optimizable Components** ğŸ”„\n",
    "   - IntegratedText2SQL: Optimize with GEPA (15-20% accuracy gain)\n",
    "   - SQLWriterModule: Optimize with Bootstrap (10-15% accuracy gain)\n",
    "   - AnswerGenerator: Fast enough without optimization\n",
    "\n",
    "4. **Transparency** ğŸ“‹\n",
    "   - Complete reasoning trace from DSPy modules\n",
    "   - Agent provides natural language flow\n",
    "   - Easy to debug and explain\n",
    "\n",
    "5. **Production Ready** ğŸš€\n",
    "   - Best tool for each job\n",
    "   - Balanced speed vs. accuracy\n",
    "   - Scalable architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f40fd",
   "metadata": {},
   "source": [
    "## Comparison: Single Agent vs Multi-Agent Architecture\n",
    "\n",
    "| Aspect | Single Agent + Tools | Multi-Agent Architecture |\n",
    "|--------|---------------------|-------------------------|\n",
    "| **Structure** | One agent with 5 tools | 4 specialized agents + orchestrator |\n",
    "| **Complexity** | Simpler setup | More complex coordination |\n",
    "| **Reasoning** | Single context window | Distributed reasoning across agents |\n",
    "| **Debugging** | Harder to isolate issues | Easier - test agents independently |\n",
    "| **Scalability** | Limited by context size | Each agent has fresh context |\n",
    "| **Parallelization** | Sequential tool calls | Potential for parallel agent execution |\n",
    "| **Error Handling** | One failure affects all | Isolated failures, easier recovery |\n",
    "| **Cost** | Lower (1 agent session) | Higher (multiple agent sessions) |\n",
    "| **Best For** | Simple workflows, cost-sensitive | Complex workflows, modularity needed |\n",
    "\n",
    "### When to Use Each:\n",
    "\n",
    "**Single Agent + Tools** (like `complete_text2sql_workflow`):\n",
    "- âœ… Simpler workflows with clear sequential steps\n",
    "- âœ… Cost-sensitive applications\n",
    "- âœ… When all tasks fit in one context window\n",
    "- âœ… Fast prototyping and testing\n",
    "\n",
    "**Multi-Agent Architecture** (like `multi_agent_text2sql_workflow`):\n",
    "- âœ… Complex workflows with many specialized tasks\n",
    "- âœ… When different tasks require different expertise\n",
    "- âœ… Need for parallel execution\n",
    "- âœ… Better error isolation and testing\n",
    "- âœ… Larger applications with team specialization\n",
    "\n",
    "### Advanced Multi-Agent Patterns:\n",
    "\n",
    "1. **Hierarchical**: Supervisor agent delegates to worker agents\n",
    "2. **Sequential**: Agents pass work in a pipeline\n",
    "3. **Parallel**: Multiple agents work simultaneously, results merged\n",
    "4. **Collaborative**: Agents discuss and reach consensus\n",
    "5. **Competitive**: Multiple agents propose solutions, best one selected\n",
    "\n",
    "For your Text-to-SQL use case, **single agent + tools is recommended** for production due to:\n",
    "- Lower cost (one agent session)\n",
    "- Sufficient for sequential workflow\n",
    "- Easier to maintain and deploy\n",
    "- Faster execution (no agent handoff overhead)\n",
    "\n",
    "Use multi-agent when you need to scale to handle dozens of different database types or complex query planning scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b40ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the multi-agent workflow\n",
    "await multi_agent_text2sql_workflow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Multi-Agent Architecture Implementation (Refactored for Clarity)\n",
    "# ============================================================================\n",
    "\n",
    "async def multi_agent_text2sql_workflow() -> None:\n",
    "    \"\"\"\n",
    "    Text-to-SQL using Multi-Agent Architecture Pattern.\n",
    "    \n",
    "    Architecture:\n",
    "    - View Selector Agent: Handles entity resolution + view selection (DSPy)\n",
    "    - Schema Expert Agent: Retrieves and explains database schemas\n",
    "    - SQL Generator Agent: Generates SQL queries (DSPy)\n",
    "    - Data Analyst Agent: Interprets results and generates answers\n",
    "    \n",
    "    The main function orchestrates communication between specialized agents.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ¤– Multi-Agent Text-to-SQL Workflow\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    query = \"What is MIC's current exposure in the USA in Private Equity?\"\n",
    "    print(f\"\\nğŸ‘¤ User Question: {query}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize Azure credentials (shared across all agents)\n",
    "    async with AzureCliCredential() as credential:\n",
    "        client = AzureAIClient(credential=credential)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 1: View Selection Agent\n",
    "        # ====================================================================\n",
    "        print(\"\\nğŸ¤– STEP 1: [View Selector Agent] Resolving entities and selecting views...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        view_selection_result = \"\"\n",
    "        async with client.create_agent(\n",
    "            name=\"ViewSelectorAgent\",\n",
    "            instructions='''You are a specialized agent for database view selection.\n",
    "\n",
    "Your expertise:\n",
    "- Resolve business entities (e.g., 'PE' â†’ 'Private Equity', 'MIC' â†’ company identifier)\n",
    "- Select optimal database views for answering questions\n",
    "- Use Chain-of-Thought reasoning via DSPy module\n",
    "\n",
    "When you receive a question:\n",
    "1. Call select_views_with_entities(question) \n",
    "2. Return a structured response with:\n",
    "   - Resolved entities (JSON format)\n",
    "   - Selected view names (list)\n",
    "   - Reasoning for your selections\n",
    "\n",
    "Be concise and precise.''',\n",
    "            tools=[select_views_with_entities],\n",
    "        ) as view_agent:\n",
    "            async for chunk in view_agent.run_stream(query):\n",
    "                if chunk.text:\n",
    "                    print(chunk.text, end=\"\", flush=True)\n",
    "                    view_selection_result += chunk.text\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 2: Schema Expert Agent\n",
    "        # ====================================================================\n",
    "        print(\"\\nğŸ¤– STEP 2: [Schema Expert Agent] Retrieving database schemas...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        schema_query = f\"Get schemas for the views needed to answer: {query}\\n\\nContext from View Selector:\\n{view_selection_result}\"\n",
    "        schema_info = \"\"\n",
    "        \n",
    "        async with client.create_agent(\n",
    "            name=\"SchemaExpertAgent\",\n",
    "            instructions='''You are a database schema expert.\n",
    "\n",
    "Your role:\n",
    "- Retrieve detailed schema information for database views\n",
    "- Explain column meanings, types, and relationships\n",
    "- Identify key columns relevant to the question\n",
    "\n",
    "When given view names:\n",
    "1. Call get_entity_schema(view_name) for each view\n",
    "2. Summarize the most relevant columns\n",
    "3. Highlight any important constraints or allowed values\n",
    "\n",
    "Format your response clearly for the SQL Generator.''',\n",
    "            tools=[get_entity_schema],\n",
    "        ) as schema_agent:\n",
    "            async for chunk in schema_agent.run_stream(schema_query):\n",
    "                if chunk.text:\n",
    "                    print(chunk.text, end=\"\", flush=True)\n",
    "                    schema_info += chunk.text\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 3: SQL Generator Agent\n",
    "        # ====================================================================\n",
    "        print(\"\\nğŸ¤– STEP 3: [SQL Generator Agent] Generating SQL query...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        sql_query_prompt = f\"\"\"Generate SQL for: {query}\n",
    "\n",
    "View Selection Context:\n",
    "{view_selection_result}\n",
    "\n",
    "Schema Information:\n",
    "{schema_info}\n",
    "\n",
    "Generate the SQL query now.\"\"\"\n",
    "        sql_result = \"\"\n",
    "        \n",
    "        async with client.create_agent(\n",
    "            name=\"SQLGeneratorAgent\",\n",
    "            instructions='''You are an expert SQL query generator for Snowflake.\n",
    "\n",
    "Your expertise:\n",
    "- Generate syntactically correct Snowflake SQL\n",
    "- Use Chain-of-Thought reasoning via DSPy module\n",
    "- Apply filters based on resolved entities\n",
    "\n",
    "When you receive:\n",
    "- Original question\n",
    "- View schemas with column details\n",
    "- Resolved entities\n",
    "\n",
    "Call generate_sql_query(question, view_schemas, resolved_entities) to create optimized SQL.\n",
    "Return the SQL query with your reasoning.''',\n",
    "            tools=[generate_sql_query],\n",
    "        ) as sql_agent:\n",
    "            async for chunk in sql_agent.run_stream(sql_query_prompt):\n",
    "                if chunk.text:\n",
    "                    print(chunk.text, end=\"\", flush=True)\n",
    "                    sql_result += chunk.text\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 4: Query Execution (Utility - not an agent)\n",
    "        # ====================================================================\n",
    "        print(\"\\nâš™ï¸  STEP 4: [Query Execution] Running SQL...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Mock execution for demo - replace with actual Snowflake connection\n",
    "        mock_results = \"\"\"\n",
    "Query Results:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ PLATFORM â”‚ COUNTRY         â”‚ UNREALIZED_VALUE  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ PE       â”‚ USA             â”‚ $2,450,000,000    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "(1 row returned)\n",
    "\"\"\"\n",
    "        print(mock_results)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 5: Data Analyst Agent\n",
    "        # ====================================================================\n",
    "        print(\"\\nğŸ¤– STEP 5: [Data Analyst Agent] Interpreting results...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        analyst_prompt = f\"\"\"Analyze these results and answer the user's question.\n",
    "\n",
    "Original Question: {query}\n",
    "\n",
    "SQL Query Context:\n",
    "{sql_result}\n",
    "\n",
    "Query Results:\n",
    "{mock_results}\n",
    "\n",
    "Provide a clear business answer.\"\"\"\n",
    "        \n",
    "        async with client.create_agent(\n",
    "            name=\"DataAnalystAgent\",\n",
    "            instructions='''You are a business data analyst.\n",
    "\n",
    "Your role:\n",
    "- Interpret SQL query results\n",
    "- Provide clear, business-friendly answers\n",
    "- Include context and insights\n",
    "\n",
    "When you receive query results:\n",
    "1. Call generate_final_answer(question, sql_query, results, entities)\n",
    "2. Provide a concise answer with:\n",
    "   - Direct answer to the question\n",
    "   - Specific numbers from results\n",
    "   - Business context\n",
    "   - Any relevant observations\n",
    "\n",
    "Be professional and concise.''',\n",
    "            tools=[generate_final_answer],\n",
    "        ) as analyst_agent:\n",
    "            async for chunk in analyst_agent.run_stream(analyst_prompt):\n",
    "                if chunk.text:\n",
    "                    print(chunk.text, end=\"\", flush=True)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"âœ… Multi-Agent Workflow Complete!\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "print(\"âœ… Multi-agent workflow function defined (refactored for clarity)\")\n",
    "print(\"\\nRun: await multi_agent_text2sql_workflow()\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a2ed5e",
   "metadata": {},
   "source": [
    "## Multi-Agent Architecture Pattern\n",
    "\n",
    "In this implementation, the **orchestrator is the Python function itself** (`multi_agent_text2sql_workflow`), not a separate agent. It coordinates the sequential workflow:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         Python Function (Orchestrator Logic)                        â”‚\n",
    "â”‚         multi_agent_text2sql_workflow()                             â”‚\n",
    "â”‚  â€¢ Receives user question                                           â”‚\n",
    "â”‚  â€¢ Creates agents sequentially                                      â”‚\n",
    "â”‚  â€¢ Passes context between agents                                    â”‚\n",
    "â”‚  â€¢ Manages workflow state                                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â”‚ Creates & Coordinates Agents\n",
    "                         â”‚ (Sequential Pipeline)\n",
    "                         â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚                â”‚                â”‚                â”‚\n",
    "        â–¼                â–¼                â–¼                â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚View Selectorâ”‚  â”‚Schema Expertâ”‚  â”‚SQL Generatorâ”‚  â”‚Data Analyst â”‚\n",
    "â”‚   Agent     â”‚  â”‚   Agent     â”‚  â”‚   Agent     â”‚  â”‚   Agent     â”‚\n",
    "â”‚  (Step 1)   â”‚  â”‚  (Step 2)   â”‚  â”‚  (Step 3)   â”‚  â”‚  (Step 5)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â”‚                â”‚                â”‚                â”‚\n",
    "        â”‚                â”‚                â”‚                â”‚\n",
    "   Uses DSPy      Fast lookup      Uses DSPy       Interprets\n",
    "IntegratedT2SQL                  SQLWriter         results\n",
    "        â”‚                â”‚                â”‚                â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                  â”‚\n",
    "                              Step 4: Query Execution\n",
    "                              (Utility function)\n",
    "```\n",
    "\n",
    "**Key Differences from Traditional Multi-Agent Systems:**\n",
    "\n",
    "1. **Orchestrator = Python Function**: \n",
    "   - Not a separate LLM agent\n",
    "   - Deterministic workflow logic\n",
    "   - Explicit control flow\n",
    "\n",
    "2. **Sequential Handoffs**:\n",
    "   - Each agent completes its task\n",
    "   - Output passed as input to next agent\n",
    "   - State managed by Python variables\n",
    "\n",
    "3. **Agent Lifecycle**:\n",
    "   - Agents created on-demand\n",
    "   - Used for single task\n",
    "   - Cleaned up after completion\n",
    "\n",
    "**Benefits:**\n",
    "1. **Predictable Flow**: Python function ensures deterministic execution order\n",
    "2. **Error Isolation**: Each agent's failure is contained in its step\n",
    "3. **Easy Testing**: Test individual agents or the entire workflow\n",
    "4. **Clear Debugging**: Print statements show exact workflow progress\n",
    "5. **Cost Efficient**: Agents only exist when needed\n",
    "\n",
    "**Alternative: True Orchestrator Agent**\n",
    "\n",
    "For dynamic workflows where the sequence isn't predetermined, you could create an actual orchestrator agent that decides which specialized agent to call based on the conversation state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f23fff",
   "metadata": {},
   "source": [
    "## âš ï¸ Important: Double LLM Calls in Multi-Agent Architecture\n",
    "\n",
    "You've identified a **critical inefficiency** in the multi-agent approach:\n",
    "\n",
    "### The Double LLM Problem:\n",
    "\n",
    "**In Multi-Agent Architecture (Approach 2):**\n",
    "```\n",
    "User Question\n",
    "    â†“\n",
    "View Selector Agent (MAF - LLM Call #1)\n",
    "    â†“ calls tool\n",
    "select_views_with_entities() â†’ DSPy Module (LLM Call #2)\n",
    "    â†“\n",
    "Result back to agent\n",
    "```\n",
    "\n",
    "**Each specialized agent makes TWO LLM calls:**\n",
    "1. **Agent LLM call**: Agent reasoning to decide which tool to call\n",
    "2. **DSPy LLM call**: DSPy module performing the actual heavy reasoning\n",
    "\n",
    "**In Single Agent Architecture (Approach 1):**\n",
    "```\n",
    "User Question\n",
    "    â†“\n",
    "Single Agent (MAF - LLM Call #1)\n",
    "    â†“ calls tool\n",
    "select_views_with_entities() â†’ DSPy Module (LLM Call #2)\n",
    "    â†“\n",
    "Result back to same agent\n",
    "```\n",
    "\n",
    "### Cost & Latency Comparison:\n",
    "\n",
    "| Architecture | LLM Calls per Task | Total LLM Calls | Agent Overhead |\n",
    "|--------------|-------------------|-----------------|----------------|\n",
    "| **Single Agent + Tools** | 2 (agent + DSPy) | ~10 calls | Low - reuses context |\n",
    "| **Multi-Agent** | 2 per agent Ã— 4 agents | ~16+ calls | High - fresh context each time |\n",
    "\n",
    "### Why Approach 1 (Single Agent) is Better:\n",
    "\n",
    "âœ… **Fewer LLM Calls**: One agent context across all steps  \n",
    "âœ… **Lower Cost**: ~40% fewer LLM invocations  \n",
    "âœ… **Faster**: No agent-to-agent handoff overhead  \n",
    "âœ… **Shared Context**: Agent maintains conversation history  \n",
    "âœ… **Simpler Debugging**: Single agent conversation to trace  \n",
    "\n",
    "### When Multi-Agent Might Make Sense:\n",
    "\n",
    "Only use multi-agent if you need:\n",
    "- **Different LLM models** per task (e.g., GPT-4o for view selection, GPT-4o-mini for schema lookup)\n",
    "- **True parallelization** of independent tasks\n",
    "- **Agent specialization** with different prompt templates and temperature settings\n",
    "- **Fault isolation** where agent failures shouldn't affect others\n",
    "\n",
    "### Recommendation for Your Use Case:\n",
    "\n",
    "**Use Single Agent + Tools (Approach 1)** because:\n",
    "1. Your workflow is strictly sequential (can't parallelize)\n",
    "2. All tasks use the same LLM model (GPT-4o)\n",
    "3. Tasks need shared context (entities, views, schemas)\n",
    "4. Cost and speed matter more than modularity\n",
    "\n",
    "The multi-agent pattern is educational but not optimal for this specific Text-to-SQL pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
