{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb332eaf",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup and Imports\n",
    "\n",
    "Import all necessary libraries for DSPy optimization and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Set\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# DSPy framework and optimizers\n",
    "import dspy\n",
    "from dspy.teleprompt import LabeledFewShot, GEPA, BootstrapFewShot\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(42)  # For reproducibility\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üì¶ DSPy version: {dspy.__version__}\")\n",
    "print(f\"üì¶ pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb2bd6",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Data and Baseline Results\n",
    "\n",
    "Load the preprocessed data and baseline performance metrics from previous notebooks.\n",
    "\n",
    "### What We're Loading:\n",
    "1. **Training/Test Examples**: From Notebook 1\n",
    "2. **Snowflake Views Metadata**: Database view definitions\n",
    "3. **Baseline Results**: Performance metrics to beat\n",
    "4. **Baseline Module**: Starting point for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path(\"data/processed\")\n",
    "BASELINE_DIR = Path(\"data/baseline\")\n",
    "OUTPUT_DIR = Path(\"data/optimization_results\")\n",
    "MODULES_DIR = Path(\"data/optimized_modules\")\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODULES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üîç LOADING DATA AND BASELINE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load training examples\n",
    "with open(DATA_DIR / \"train_examples.json\", 'r', encoding='utf-8') as f:\n",
    "    train_examples = json.load(f)\n",
    "print(f\"‚úÖ Loaded {len(train_examples)} training examples\")\n",
    "\n",
    "# Load test examples\n",
    "with open(DATA_DIR / \"test_examples.json\", 'r', encoding='utf-8') as f:\n",
    "    test_examples = json.load(f)\n",
    "print(f\"‚úÖ Loaded {len(test_examples)} test examples\")\n",
    "\n",
    "# Load Snowflake views metadata\n",
    "with open(DATA_DIR / \"snowflake_views_metadata.json\", 'r', encoding='utf-8') as f:\n",
    "    snowflake_views = json.load(f)\n",
    "print(f\"‚úÖ Loaded {len(snowflake_views)} Snowflake views\")\n",
    "\n",
    "# Load baseline metrics\n",
    "with open(BASELINE_DIR / \"baseline_metrics.json\", 'r', encoding='utf-8') as f:\n",
    "    baseline_metrics = json.load(f)\n",
    "print(f\"‚úÖ Loaded baseline metrics\")\n",
    "print(f\"   ‚Ä¢ Baseline Accuracy: {baseline_metrics['accuracy']:.3f}\")\n",
    "print(f\"   ‚Ä¢ Baseline F1: {baseline_metrics['f1']:.3f}\")\n",
    "\n",
    "# Load baseline module (optional, we'll create fresh ones)\n",
    "try:\n",
    "    with open(BASELINE_DIR / \"baseline_view_selector.pkl\", 'rb') as f:\n",
    "        baseline_module = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded baseline module\")\n",
    "except:\n",
    "    print(f\"‚ö†Ô∏è  Could not load baseline module (will create fresh ones)\")\n",
    "    baseline_module = None\n",
    "\n",
    "# Combine for evaluation\n",
    "all_examples = train_examples + test_examples\n",
    "\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"  ‚Ä¢ Training examples: {len(train_examples)}\")\n",
    "print(f\"  ‚Ä¢ Test examples: {len(test_examples)}\")\n",
    "print(f\"  ‚Ä¢ Total examples: {len(all_examples)}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ Data loading complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce119b",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configure DSPy\n",
    "\n",
    "Set up DSPy with Azure OpenAI models for optimization.\n",
    "\n",
    "### Configuration:\n",
    "- **Main Model**: GPT-4o for view selection\n",
    "- **Reflection Model**: GPT-4.1 for GEPA reflection\n",
    "- **Temperature**: 0.0 for main model (deterministic), 1.0 for reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è CONFIGURING DSPY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Main LM for view selection\n",
    "    lm = dspy.LM(\n",
    "        model=\"azure/gpt-4o\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=2000\n",
    "    )\n",
    "    dspy.configure(lm=lm)\n",
    "    print(\"‚úÖ Main LM configured (gpt-4o)\")\n",
    "    \n",
    "    # Reflection LM for GEPA optimizer\n",
    "    reflection_lm = dspy.LM(\n",
    "        model='azure/gpt-4.1',\n",
    "        temperature=1.0,  # Higher temperature for creative reflection\n",
    "        max_tokens=10000\n",
    "    )\n",
    "    print(\"‚úÖ Reflection LM configured (gpt-4.1)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Failed to configure DSPy: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ DSPy configuration complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5e818",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Define ViewSelector Module\n",
    "\n",
    "Recreate the ViewSelectorModule for optimization (same as Notebook 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewSelectorSignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Database view selector with Chain-of-Thought reasoning.\n",
    "    \"\"\"\n",
    "    # Input fields\n",
    "    question: str = dspy.InputField(\n",
    "        desc=\"User's natural language database query\"\n",
    "    )\n",
    "    candidate_views: list = dspy.InputField(\n",
    "        desc=\"List of available database views with descriptions, selectors, and columns\"\n",
    "    )\n",
    "    conversation_history: str = dspy.InputField(\n",
    "        desc=\"Previous conversation context for resolving references\",\n",
    "        default=\"\"\n",
    "    )\n",
    "    domain_knowledge: str = dspy.InputField(\n",
    "        desc=\"Financial domain rules (Asset Classes, Investment Classes, Platforms)\"\n",
    "    )\n",
    "    # Output fields\n",
    "    reasoning: str = dspy.OutputField(\n",
    "        desc=\"Step-by-step analysis of why specific views were selected\"\n",
    "    )\n",
    "    selected_views: list = dspy.OutputField(\n",
    "        desc=\"List of selected view entity names, or ['<NO_VIEWS>'] if none are relevant\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ViewSelectorModule(dspy.Module):\n",
    "    \"\"\"\n",
    "    Baseline database view selector using DSPy Chain-of-Thought reasoning.\n",
    "    \"\"\"\n",
    "    def __init__(self, candidate_views: List[Dict] = None):\n",
    "        super().__init__()\n",
    "        self.selector_cot = dspy.ChainOfThought(ViewSelectorSignature)\n",
    "        self.domain_knowledge = \"\"\"\n",
    "CRITICAL FINANCIAL CLASSIFICATION RULES:\n",
    "\n",
    "1. INVESTMENT CLASSES:\n",
    "   - Available values: Private Equity, Real Estate, Infrastructure, Credit, Hedge Funds\n",
    "\n",
    "2. ASSET CLASSES:\n",
    "   - Available values: Private Equity, Real Estate, Infrastructure, Credit, Hedge Funds\n",
    "   - Note: PE refers to Private Equity (an ASSET CLASS)\n",
    "\n",
    "3. PLATFORMS:\n",
    "   - Available platform values: Private Equity, Real Assets, ADIC, UAE Investments, \n",
    "     Credit and Special Situations\n",
    "        \"\"\"\n",
    "        self.candidate_views = candidate_views or []\n",
    "    \n",
    "    def forward(self, question: str, conversation_history: str = \"\"):\n",
    "        cot_result = self.selector_cot(\n",
    "            question=question,\n",
    "            candidate_views=self.candidate_views,\n",
    "            conversation_history=conversation_history,\n",
    "            domain_knowledge=self.domain_knowledge\n",
    "        )\n",
    "        return cot_result\n",
    "\n",
    "print(\"‚úÖ ViewSelectorModule defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cd84d",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Define Evaluation Metrics\n",
    "\n",
    "Create strict and soft metrics for DSPy optimization.\n",
    "\n",
    "### Metrics:\n",
    "1. **Strict Metric**: Exact match (1.0 or 0.0) - used for critical optimization\n",
    "2. **Soft Metric**: Jaccard similarity - allows partial credit\n",
    "\n",
    "### Important:\n",
    "Both metrics use **5-argument signature** required by GEPA optimizer:\n",
    "`(gold, pred, trace=None, pred_name=None, pred_trace=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17191225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strict_view_selector_metric(gold, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    \"\"\"\n",
    "    Strict evaluation metric for view selection that returns 1.0 for exact matches, 0.0 otherwise.\n",
    "    \n",
    "    Args:\n",
    "        gold: Expected/ground truth example\n",
    "        pred: Predicted output\n",
    "        trace: Optional trace information\n",
    "        pred_name: Optional predictor name\n",
    "        pred_trace: Optional predictor trace\n",
    "    \n",
    "    Returns:\n",
    "        1.0 for exact match, 0.0 otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract predicted views from the prediction\n",
    "        predicted_views = getattr(pred, 'selected_views', [])\n",
    "        # Extract gold/expected views \n",
    "        expected_views = getattr(gold, 'selected_views', [])\n",
    "        \n",
    "        # Normalize both to sets for comparison (order doesn't matter)\n",
    "        if isinstance(predicted_views, str):\n",
    "            predicted_views = [v.strip() for v in predicted_views.split(',') if v.strip()]\n",
    "        elif not isinstance(predicted_views, list):\n",
    "            predicted_views = [str(predicted_views)]\n",
    "            \n",
    "        if isinstance(expected_views, str):\n",
    "            expected_views = [v.strip() for v in expected_views.split(',') if v.strip()]\n",
    "        elif not isinstance(expected_views, list):\n",
    "            expected_views = [str(expected_views)]\n",
    "        \n",
    "        # Convert to sets for order-independent comparison\n",
    "        pred_set = set(predicted_views)\n",
    "        expected_set = set(expected_views)\n",
    "        \n",
    "        # Return 1.0 for exact match, 0.0 otherwise (strict)\n",
    "        return 1.0 if pred_set == expected_set else 0.0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in strict metric: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def soft_view_selector_metric(gold, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    \"\"\"\n",
    "    Soft evaluation metric using Jaccard similarity for partial credit.\n",
    "    \n",
    "    Args:\n",
    "        gold: Expected/ground truth example\n",
    "        pred: Predicted output\n",
    "        trace: Optional trace information\n",
    "        pred_name: Optional predictor name\n",
    "        pred_trace: Optional predictor trace\n",
    "    \n",
    "    Returns:\n",
    "        Jaccard similarity score (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract predicted views from the prediction\n",
    "        predicted_views = getattr(pred, 'selected_views', [])\n",
    "        # Extract gold/expected views \n",
    "        expected_views = getattr(gold, 'selected_views', [])\n",
    "        \n",
    "        # Normalize both to sets for comparison\n",
    "        if isinstance(predicted_views, str):\n",
    "            predicted_views = [v.strip() for v in predicted_views.split(',') if v.strip()]\n",
    "        elif not isinstance(predicted_views, list):\n",
    "            predicted_views = [str(predicted_views)]\n",
    "            \n",
    "        if isinstance(expected_views, str):\n",
    "            expected_views = [v.strip() for v in expected_views.split(',') if v.strip()]\n",
    "        elif not isinstance(expected_views, list):\n",
    "            expected_views = [str(expected_views)]\n",
    "        \n",
    "        # Convert to sets\n",
    "        pred_set = set(predicted_views)\n",
    "        expected_set = set(expected_views)\n",
    "\n",
    "        # Handle empty sets\n",
    "        if not pred_set and not expected_set:\n",
    "            return 1.0  # both empty = perfect match\n",
    "        if not expected_set:\n",
    "            return 0.0  # avoid div-by-zero\n",
    "        \n",
    "        # Jaccard Similarity: intersection / union\n",
    "        intersection = len(pred_set & expected_set)\n",
    "        union = len(pred_set | expected_set)\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Soft metric error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "print(\"‚úÖ Created evaluation metrics:\")\n",
    "print(\"   ‚Ä¢ strict_view_selector_metric: Exact match (1.0/0.0)\")\n",
    "print(\"   ‚Ä¢ soft_view_selector_metric: Jaccard similarity (0.0-1.0)\")\n",
    "print(\"   ‚Ä¢ Both use 5-argument signature for GEPA compatibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a60b5",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Prepare DSPy Training Dataset\n",
    "\n",
    "Convert training examples into DSPy-compatible format.\n",
    "\n",
    "### Process:\n",
    "1. Create `dspy.Example` objects with proper fields\n",
    "2. Mark input fields (question, conversation_history)\n",
    "3. Mark output fields (selected_views)\n",
    "4. Split into train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß CREATING DSPY TRAINING DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_dspy_examples = []\n",
    "\n",
    "for i, ex in enumerate(train_examples + test_examples):\n",
    "    try:\n",
    "        # Parse expected views\n",
    "        expected_views = ex.get('expected_views', '')\n",
    "        if isinstance(expected_views, str):\n",
    "            if expected_views.strip() == '<NO_VIEWS>' or not expected_views.strip():\n",
    "                views_list = ['<NO_VIEWS>']\n",
    "            else:\n",
    "                views_list = [v.strip() for v in expected_views.split(',') if v.strip()]\n",
    "        else:\n",
    "            views_list = expected_views if isinstance(expected_views, list) else []\n",
    "        \n",
    "        # Create DSPy Example\n",
    "        dspy_example = dspy.Example(\n",
    "            question=ex['question'],\n",
    "            conversation_history=ex.get('conversation_history', ''),\n",
    "            selected_views=views_list\n",
    "        ).with_inputs('question', 'conversation_history')\n",
    "        \n",
    "        all_dspy_examples.append(dspy_example)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Skipping example {i}: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Created {len(all_dspy_examples)} DSPy examples\")\n",
    "\n",
    "# Split for training and validation\n",
    "shuffled = all_dspy_examples.copy()\n",
    "random.shuffle(shuffled)\n",
    "\n",
    "# Split 70/30\n",
    "split_ratio = 0.7\n",
    "split_index = int(len(shuffled) * split_ratio)\n",
    "\n",
    "train_dspy = shuffled[:split_index]\n",
    "val_dspy = shuffled[split_index:]\n",
    "\n",
    "print(f\"üìä Training examples: {len(train_dspy)}\")\n",
    "print(f\"üìä Validation examples: {len(val_dspy)}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nüìã Sample Training Example:\")\n",
    "print(f\"   Question: {train_dspy[0].question[:80]}...\")\n",
    "print(f\"   Expected views: {train_dspy[0].selected_views}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ DSPy dataset preparation complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b38395d",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Section 3A: LabeledFewShot Optimization\n",
    "\n",
    "Apply LabeledFewShot optimizer - the simplest optimization approach.\n",
    "\n",
    "### How It Works:\n",
    "- Selects best k examples from training data\n",
    "- Uses them as few-shot demonstrations\n",
    "- No additional training or reflection\n",
    "\n",
    "### Advantages:\n",
    "- Fast and simple\n",
    "- Low cost\n",
    "- Easy to understand\n",
    "\n",
    "### Configuration:\n",
    "- k=10 few-shot examples\n",
    "- Diverse example selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4921a51",
   "metadata": {},
   "source": [
    "### 7.1 Configure and Compile LabeledFewShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ SECTION 3A: LABELEDFEWSHOT OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create fresh module instance\n",
    "labeledfewshot_student = ViewSelectorModule(candidate_views=snowflake_views)\n",
    "\n",
    "print(\"‚öôÔ∏è Creating LabeledFewShot optimizer...\")\n",
    "print(f\"   ‚Ä¢ k=10 few-shot examples\")\n",
    "print(f\"   ‚Ä¢ Training set: {len(train_dspy)} examples\")\n",
    "\n",
    "# Create optimizer\n",
    "labeledfewshot_optimizer = LabeledFewShot(k=10)\n",
    "\n",
    "print(\"\\nüîß Compiling with LabeledFewShot...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    labeledfewshot_module = labeledfewshot_optimizer.compile(\n",
    "        student=labeledfewshot_student,\n",
    "        trainset=train_dspy\n",
    "    )\n",
    "    \n",
    "    compilation_time = time.time() - start_time\n",
    "    print(f\"‚úÖ LabeledFewShot optimization completed in {compilation_time:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LabeledFewShot optimization failed: {e}\")\n",
    "    labeledfewshot_module = None\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def14ba6",
   "metadata": {},
   "source": [
    "### 7.2 Evaluate LabeledFewShot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0a260",
   "metadata": {},
   "outputs": [],
   "source": [
    "if labeledfewshot_module:\n",
    "    print(\"üìä EVALUATING LABELEDFEWSHOT MODULE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    labeledfewshot_predictions = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, example in enumerate(all_examples, 1):\n",
    "        print(f\"\\rProcessing {i}/{len(all_examples)}...\", end='', flush=True)\n",
    "        \n",
    "        try:\n",
    "            prediction = labeledfewshot_module(\n",
    "                question=example['question'],\n",
    "                conversation_history=example.get('conversation_history', '')\n",
    "            )\n",
    "            \n",
    "            labeledfewshot_predictions.append({\n",
    "                'question_id': example.get('question_id', f'q_{i}'),\n",
    "                'question': example['question'],\n",
    "                'expected_views': example.get('expected_views', ''),\n",
    "                'predicted_views': prediction.selected_views,\n",
    "                'reasoning': prediction.reasoning,\n",
    "                'error': None\n",
    "            })\n",
    "        except Exception as e:\n",
    "            labeledfewshot_predictions.append({\n",
    "                'question_id': example.get('question_id', f'q_{i}'),\n",
    "                'question': example['question'],\n",
    "                'expected_views': example.get('expected_views', ''),\n",
    "                'predicted_views': [],\n",
    "                'reasoning': '',\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    eval_time = time.time() - start_time\n",
    "    print(f\"\\n\\n‚úÖ Evaluation complete in {eval_time:.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Average time per example: {eval_time/len(all_examples):.2f}s\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping evaluation - optimization failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a13be",
   "metadata": {},
   "source": [
    "### 7.3 Calculate LabeledFewShot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if labeledfewshot_module:\n",
    "    # Helper function for metrics\n",
    "    def normalize_views(views) -> Set[str]:\n",
    "        if not views:\n",
    "            return set()\n",
    "        if isinstance(views, list):\n",
    "            return set(str(v).strip() for v in views if v and str(v).strip() != '<NO_VIEWS>')\n",
    "        if isinstance(views, str):\n",
    "            if views == '<NO_VIEWS>' or not views.strip():\n",
    "                return set()\n",
    "            return set(v.strip() for v in views.split(',') if v.strip() and v.strip() != '<NO_VIEWS>')\n",
    "        return set()\n",
    "\n",
    "    def calculate_metrics(predicted_views, expected_views):\n",
    "        pred_set = normalize_views(predicted_views)\n",
    "        exp_set = normalize_views(expected_views)\n",
    "        \n",
    "        exact_match = 1.0 if pred_set == exp_set else 0.0\n",
    "        \n",
    "        if not pred_set and not exp_set:\n",
    "            return {'exact_match': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'jaccard': 1.0}\n",
    "        if not exp_set:\n",
    "            return {'exact_match': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'jaccard': 0.0}\n",
    "        \n",
    "        intersection = pred_set & exp_set\n",
    "        precision = len(intersection) / len(pred_set) if pred_set else 0.0\n",
    "        recall = len(intersection) / len(exp_set) if exp_set else 0.0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        union = pred_set | exp_set\n",
    "        jaccard = len(intersection) / len(union) if union else 0.0\n",
    "        \n",
    "        return {'exact_match': exact_match, 'precision': precision, 'recall': recall, 'f1': f1, 'jaccard': jaccard}\n",
    "    \n",
    "    # Calculate metrics for each prediction\n",
    "    for pred in labeledfewshot_predictions:\n",
    "        if pred['error'] is None:\n",
    "            metrics = calculate_metrics(pred['predicted_views'], pred['expected_views'])\n",
    "            pred.update(metrics)\n",
    "        else:\n",
    "            pred.update({'exact_match': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'jaccard': 0.0})\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    labeledfewshot_metrics = {\n",
    "        'optimizer': 'LabeledFewShot',\n",
    "        'total_examples': len(all_examples),\n",
    "        'successful_predictions': len([p for p in labeledfewshot_predictions if p['error'] is None]),\n",
    "        'accuracy': np.mean([p['exact_match'] for p in labeledfewshot_predictions]),\n",
    "        'precision': np.mean([p['precision'] for p in labeledfewshot_predictions]),\n",
    "        'recall': np.mean([p['recall'] for p in labeledfewshot_predictions]),\n",
    "        'f1': np.mean([p['f1'] for p in labeledfewshot_predictions]),\n",
    "        'jaccard': np.mean([p['jaccard'] for p in labeledfewshot_predictions]),\n",
    "        'compilation_time': compilation_time,\n",
    "        'evaluation_time': eval_time\n",
    "    }\n",
    "    \n",
    "    print(\"üìà LABELEDFEWSHOT PERFORMANCE:\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {labeledfewshot_metrics['accuracy']:.3f} ({labeledfewshot_metrics['accuracy']*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Precision: {labeledfewshot_metrics['precision']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Recall: {labeledfewshot_metrics['recall']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ F1 Score: {labeledfewshot_metrics['f1']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Jaccard: {labeledfewshot_metrics['jaccard']:.3f}\")\n",
    "    \n",
    "    # Compare to baseline\n",
    "    improvement = labeledfewshot_metrics['accuracy'] - baseline_metrics['accuracy']\n",
    "    print(f\"\\nüìä vs Baseline:\")\n",
    "    print(f\"   ‚Ä¢ Accuracy improvement: {improvement:+.3f} ({improvement*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b005703",
   "metadata": {},
   "source": [
    "### 7.4 Save LabeledFewShot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b17672",
   "metadata": {},
   "outputs": [],
   "source": [
    "if labeledfewshot_module:\n",
    "    # Save module\n",
    "    module_path = MODULES_DIR / \"labeledfewshot_module.pkl\"\n",
    "    try:\n",
    "        with open(module_path, 'wb') as f:\n",
    "            pickle.dump(labeledfewshot_module, f)\n",
    "        print(f\"‚úÖ Saved module to: {module_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not save module: {e}\")\n",
    "    \n",
    "    # Save predictions\n",
    "    results_path = OUTPUT_DIR / \"labeledfewshot_results.json\"\n",
    "    with open(results_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(labeledfewshot_predictions, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Saved results to: {results_path}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_path = OUTPUT_DIR / \"labeledfewshot_metrics.json\"\n",
    "    with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(labeledfewshot_metrics, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Saved metrics to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f844a4f",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Section 3B: GEPA Optimization\n",
    "\n",
    "Apply GEPA (Generative Error-driven Prompt Adaptation) optimizer.\n",
    "\n",
    "### How It Works:\n",
    "- Uses reflection LM to analyze errors\n",
    "- Adapts prompts based on failure patterns\n",
    "- Iteratively improves through self-reflection\n",
    "\n",
    "### Advantages:\n",
    "- More sophisticated than few-shot\n",
    "- Learns from mistakes\n",
    "- Can discover better prompts\n",
    "\n",
    "### Disadvantages:\n",
    "- Slower (requires reflection)\n",
    "- Higher cost (more API calls)\n",
    "- More complex\n",
    "\n",
    "### Configuration:\n",
    "- Reflection LM: GPT-4.1 (temperature=1.0)\n",
    "- Metric: Soft (Jaccard) for gradual improvement\n",
    "- Training: 15 examples\n",
    "- Validation: 10 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7473058e",
   "metadata": {},
   "source": [
    "### 8.1 Diagnostic Checks for GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b07c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç GEPA PRE-FLIGHT DIAGNOSTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create fresh module for GEPA\n",
    "gepa_student = ViewSelectorModule(candidate_views=snowflake_views)\n",
    "\n",
    "# Test module output format\n",
    "test_example = train_dspy[0]\n",
    "print(f\"Testing with question: {test_example.question[:60]}...\")\n",
    "\n",
    "test_result = gepa_student(\n",
    "    question=test_example.question,\n",
    "    conversation_history=test_example.conversation_history\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Result type: {type(test_result)}\")\n",
    "print(f\"‚úÖ Has 'selected_views': {hasattr(test_result, 'selected_views')}\")\n",
    "print(f\"   Selected: {test_result.selected_views}\")\n",
    "\n",
    "# Test metric\n",
    "score = soft_view_selector_metric(gold=test_example, pred=test_result)\n",
    "print(f\"\\nüìä Metric score: {score:.2f}\")\n",
    "\n",
    "if score > 0:\n",
    "    print(\"‚úÖ Module and metric working correctly!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Score is 0.0 - check if expected views are in candidate views\")\n",
    "\n",
    "# Check training data quality\n",
    "print(f\"\\nüìä Checking first 5 training examples:\")\n",
    "baseline_scores = []\n",
    "for i, ex in enumerate(train_dspy[:5], 1):\n",
    "    result = gepa_student(question=ex.question, conversation_history=ex.conversation_history)\n",
    "    score = soft_view_selector_metric(gold=ex, pred=result)\n",
    "    baseline_scores.append(score)\n",
    "    print(f\"   {i}. Score: {score:.2f} {'‚úÖ' if score > 0 else '‚ùå'}\")\n",
    "\n",
    "avg_baseline = np.mean(baseline_scores)\n",
    "print(f\"\\nüìä Average baseline score: {avg_baseline:.2f}\")\n",
    "\n",
    "if avg_baseline > 0:\n",
    "    print(\"‚úÖ Training data looks good - GEPA can optimize from here\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  All scores are 0.0 - GEPA may struggle to improve\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a053799",
   "metadata": {},
   "source": [
    "### 8.2 Configure and Compile GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef2bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† SECTION 3B: GEPA OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"‚öôÔ∏è Creating GEPA optimizer...\")\n",
    "print(f\"   ‚Ä¢ Metric: soft_view_selector_metric (Jaccard)\")\n",
    "print(f\"   ‚Ä¢ Reflection LM: gpt-4.1 (temperature=1.0)\")\n",
    "print(f\"   ‚Ä¢ Training set: 15 examples\")\n",
    "print(f\"   ‚Ä¢ Validation set: 10 examples\")\n",
    "\n",
    "# Create GEPA optimizer\n",
    "gepa_optimizer = GEPA(\n",
    "    metric=soft_view_selector_metric,\n",
    "    reflection_lm=reflection_lm,\n",
    "    num_threads=1,\n",
    "    max_full_evals=5\n",
    ")\n",
    "\n",
    "print(\"\\nüîß Compiling with GEPA optimizer...\")\n",
    "print(\"   ‚è≥ This may take several minutes (reflection requires multiple LLM calls)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    gepa_module = gepa_optimizer.compile(\n",
    "        student=gepa_student,\n",
    "        trainset=train_dspy[:15],\n",
    "        valset=val_dspy[:10]\n",
    "    )\n",
    "    \n",
    "    compilation_time_gepa = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ GEPA optimization completed in {compilation_time_gepa:.2f}s\")\n",
    "    print(f\"   ‚Ä¢ {compilation_time_gepa/60:.1f} minutes\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå GEPA optimization failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    gepa_module = None\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c66b9",
   "metadata": {},
   "source": [
    "### 8.3 Evaluate GEPA Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d4f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gepa_module:\n",
    "    print(\"üìä EVALUATING GEPA MODULE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    gepa_predictions = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, example in enumerate(all_examples, 1):\n",
    "        print(f\"\\rProcessing {i}/{len(all_examples)}...\", end='', flush=True)\n",
    "        \n",
    "        try:\n",
    "            prediction = gepa_module(\n",
    "                question=example['question'],\n",
    "                conversation_history=example.get('conversation_history', '')\n",
    "            )\n",
    "            \n",
    "            gepa_predictions.append({\n",
    "                'question_id': example.get('question_id', f'q_{i}'),\n",
    "                'question': example['question'],\n",
    "                'expected_views': example.get('expected_views', ''),\n",
    "                'predicted_views': prediction.selected_views,\n",
    "                'reasoning': prediction.reasoning,\n",
    "                'error': None\n",
    "            })\n",
    "        except Exception as e:\n",
    "            gepa_predictions.append({\n",
    "                'question_id': example.get('question_id', f'q_{i}'),\n",
    "                'question': example['question'],\n",
    "                'expected_views': example.get('expected_views', ''),\n",
    "                'predicted_views': [],\n",
    "                'reasoning': '',\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    eval_time_gepa = time.time() - start_time\n",
    "    print(f\"\\n\\n‚úÖ Evaluation complete in {eval_time_gepa:.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Average time per example: {eval_time_gepa/len(all_examples):.2f}s\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping evaluation - optimization failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dabc09d",
   "metadata": {},
   "source": [
    "### 8.4 Calculate GEPA Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34473a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gepa_module:\n",
    "    # Calculate metrics for each prediction\n",
    "    for pred in gepa_predictions:\n",
    "        if pred['error'] is None:\n",
    "            metrics = calculate_metrics(pred['predicted_views'], pred['expected_views'])\n",
    "            pred.update(metrics)\n",
    "        else:\n",
    "            pred.update({'exact_match': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'jaccard': 0.0})\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    gepa_metrics = {\n",
    "        'optimizer': 'GEPA',\n",
    "        'total_examples': len(all_examples),\n",
    "        'successful_predictions': len([p for p in gepa_predictions if p['error'] is None]),\n",
    "        'accuracy': np.mean([p['exact_match'] for p in gepa_predictions]),\n",
    "        'precision': np.mean([p['precision'] for p in gepa_predictions]),\n",
    "        'recall': np.mean([p['recall'] for p in gepa_predictions]),\n",
    "        'f1': np.mean([p['f1'] for p in gepa_predictions]),\n",
    "        'jaccard': np.mean([p['jaccard'] for p in gepa_predictions]),\n",
    "        'compilation_time': compilation_time_gepa,\n",
    "        'evaluation_time': eval_time_gepa\n",
    "    }\n",
    "    \n",
    "    print(\"üìà GEPA PERFORMANCE:\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {gepa_metrics['accuracy']:.3f} ({gepa_metrics['accuracy']*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Precision: {gepa_metrics['precision']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Recall: {gepa_metrics['recall']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ F1 Score: {gepa_metrics['f1']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Jaccard: {gepa_metrics['jaccard']:.3f}\")\n",
    "    \n",
    "    # Compare to baseline and LabeledFewShot\n",
    "    improvement_baseline = gepa_metrics['accuracy'] - baseline_metrics['accuracy']\n",
    "    print(f\"\\nüìä vs Baseline:\")\n",
    "    print(f\"   ‚Ä¢ Accuracy improvement: {improvement_baseline:+.3f} ({improvement_baseline*100:+.1f}%)\")\n",
    "    \n",
    "    if labeledfewshot_module:\n",
    "        improvement_lfs = gepa_metrics['accuracy'] - labeledfewshot_metrics['accuracy']\n",
    "        print(f\"\\nüìä vs LabeledFewShot:\")\n",
    "        print(f\"   ‚Ä¢ Accuracy difference: {improvement_lfs:+.3f} ({improvement_lfs*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5acc428",
   "metadata": {},
   "source": [
    "### 8.5 Save GEPA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gepa_module:\n",
    "    # Save module\n",
    "    module_path = MODULES_DIR / \"gepa_module.pkl\"\n",
    "    try:\n",
    "        with open(module_path, 'wb') as f:\n",
    "            pickle.dump(gepa_module, f)\n",
    "        print(f\"‚úÖ Saved module to: {module_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not save module: {e}\")\n",
    "    \n",
    "    # Save predictions\n",
    "    results_path = OUTPUT_DIR / \"gepa_results.json\"\n",
    "    with open(results_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(gepa_predictions, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Saved results to: {results_path}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_path = OUTPUT_DIR / \"gepa_metrics.json\"\n",
    "    with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(gepa_metrics, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Saved metrics to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4608ce95",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Section 3C: BootstrapFewShot Optimization\n",
    "\n",
    "Apply BootstrapFewShot optimizer - generates synthetic examples.\n",
    "\n",
    "### How It Works:\n",
    "- Uses existing examples to bootstrap new ones\n",
    "- Generates synthetic training data\n",
    "- Combines labeled and bootstrapped examples\n",
    "\n",
    "### Advantages:\n",
    "- Can expand limited training data\n",
    "- Discovers edge cases\n",
    "- More robust than simple few-shot\n",
    "\n",
    "### Disadvantages:\n",
    "- Can introduce noise\n",
    "- Slower than LabeledFewShot\n",
    "- May overfit to generated examples\n",
    "\n",
    "### Configuration:\n",
    "- max_bootstrapped_demos=8\n",
    "- max_labeled_demos=8\n",
    "- max_rounds=5\n",
    "- metric: strict (exact match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581d8cd",
   "metadata": {},
   "source": [
    "### 9.1 Configure and Compile BootstrapFewShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015652f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ SECTION 3C: BOOTSTRAPFEWSHOT OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create fresh module instance\n",
    "bootstrap_student = ViewSelectorModule(candidate_views=snowflake_views)\n",
    "\n",
    "print(\"‚öôÔ∏è Creating BootstrapFewShot optimizer...\")\n",
    "print(f\"   ‚Ä¢ Metric: strict_view_selector_metric (exact match)\")\n",
    "print(f\"   ‚Ä¢ max_bootstrapped_demos=8\")\n",
    "print(f\"   ‚Ä¢ max_labeled_demos=8\")\n",
    "print(f\"   ‚Ä¢ max_rounds=5\")\n",
    "\n",
    "# Create optimizer\n",
    "bootstrap_optimizer = BootstrapFewShot(\n",
    "    metric=strict_view_selector_metric,\n",
    "    max_bootstrapped_demos=8,\n",
    "    max_labeled_demos=8,\n",
    "    max_rounds=5,\n",
    "    max_errors=1\n",
    ")\n",
    "\n",
    "print(\"\\nüîß Compiling with BootstrapFewShot...\")\n",
    "print(\"   ‚è≥ This may take a few minutes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    bootstrap_module = bootstrap_optimizer.compile(\n",
    "        student=bootstrap_student,\n",
    "        trainset=train_dspy\n",
    "    )\n",
    "    \n",
    "    compilation_time_bootstrap = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ BootstrapFewShot optimization completed in {compilation_time_bootstrap:.2f}s\")\n",
    "    print(f\"   ‚Ä¢ {compilation_time_bootstrap/60:.1f} minutes\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå BootstrapFewShot optimization failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    bootstrap_module = None\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946cf0d",
   "metadata": {},
   "source": [
    "### 9.2 Evaluate BootstrapFewShot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e34cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bootstrap_module:\n",
    "    print(\"üìä EVALUATING BOOTSTRAPFEWSHOT MODULE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    bootstrap_predictions = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, example in enumerate(all_examples, 1):\n",
    "        print(f\"\\rProcessing {i}/{len(all_examples)}...\", end='', flush=True)\n",
    "        \n",
    "        try:\n",
    "            prediction = bootstrap_module(\n",
    "                question=example['question'],\n",
    "                conversation_history=example.get('conversation_history', '')\n",
    "            )\n",
    "            \n",
    "            bootstrap_predictions.append({\n",
    "                'question_id': example.get('question_id', f'q_{i}'),\n",
    "                'question': example['question'],\n",
    "                'expected_views': example.get('expected_views', ''),\n",
    "                'predicted_views': prediction.selected_views,\n",
    "                'reasoning': prediction.reasoning,\n",
    "                'error': None\n",
    "            })\n",
    "        except Exception as e:\n",
    "            bootstrap_predictions.append({\n",
    "                'question_id': example.get('question_id', f'q_{i}'),\n",
    "                'question': example['question'],\n",
    "                'expected_views': example.get('expected_views', ''),\n",
    "                'predicted_views': [],\n",
    "                'reasoning': '',\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    eval_time_bootstrap = time.time() - start_time\n",
    "    print(f\"\\n\\n‚úÖ Evaluation complete in {eval_time_bootstrap:.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Average time per example: {eval_time_bootstrap/len(all_examples):.2f}s\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping evaluation - optimization failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06473de5",
   "metadata": {},
   "source": [
    "### 9.3 Calculate BootstrapFewShot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9542f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bootstrap_module:\n",
    "    # Calculate metrics for each prediction\n",
    "    for pred in bootstrap_predictions:\n",
    "        if pred['error'] is None:\n",
    "            metrics = calculate_metrics(pred['predicted_views'], pred['expected_views'])\n",
    "            pred.update(metrics)\n",
    "        else:\n",
    "            pred.update({'exact_match': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'jaccard': 0.0})\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    bootstrap_metrics = {\n",
    "        'optimizer': 'BootstrapFewShot',\n",
    "        'total_examples': len(all_examples),\n",
    "        'successful_predictions': len([p for p in bootstrap_predictions if p['error'] is None]),\n",
    "        'accuracy': np.mean([p['exact_match'] for p in bootstrap_predictions]),\n",
    "        'precision': np.mean([p['precision'] for p in bootstrap_predictions]),\n",
    "        'recall': np.mean([p['recall'] for p in bootstrap_predictions]),\n",
    "        'f1': np.mean([p['f1'] for p in bootstrap_predictions]),\n",
    "        'jaccard': np.mean([p['jaccard'] for p in bootstrap_predictions]),\n",
    "        'compilation_time': compilation_time_bootstrap,\n",
    "        'evaluation_time': eval_time_bootstrap\n",
    "    }\n",
    "    \n",
    "    print(\"üìà BOOTSTRAPFEWSHOT PERFORMANCE:\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {bootstrap_metrics['accuracy']:.3f} ({bootstrap_metrics['accuracy']*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Precision: {bootstrap_metrics['precision']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Recall: {bootstrap_metrics['recall']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ F1 Score: {bootstrap_metrics['f1']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Jaccard: {bootstrap_metrics['jaccard']:.3f}\")\n",
    "    \n",
    "    # Compare to baseline\n",
    "    improvement = bootstrap_metrics['accuracy'] - baseline_metrics['accuracy']\n",
    "    print(f\"\\nüìä vs Baseline:\")\n",
    "    print(f\"   ‚Ä¢ Accuracy improvement: {improvement:+.3f} ({improvement*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7678931",
   "metadata": {},
   "source": [
    "### 9.4 Save BootstrapFewShot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3266bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bootstrap_module:\n",
    "    # Save module\n",
    "    module_path = MODULES_DIR / \"bootstrap_module.pkl\"\n",
    "    try:\n",
    "        with open(module_path, 'wb') as f:\n",
    "            pickle.dump(bootstrap_module, f)\n",
    "        print(f\"‚úÖ Saved module to: {module_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not save module: {e}\")\n",
    "    \n",
    "    # Save predictions\n",
    "    results_path = OUTPUT_DIR / \"bootstrap_results.json\"\n",
    "    with open(results_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(bootstrap_predictions, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Saved results to: {results_path}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_path = OUTPUT_DIR / \"bootstrap_metrics.json\"\n",
    "    with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(bootstrap_metrics, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Saved metrics to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406596f6",
   "metadata": {},
   "source": [
    "## üîü Section 3D: Comprehensive Comparison\n",
    "\n",
    "Compare all optimizers side-by-side.\n",
    "\n",
    "### Comparison Dimensions:\n",
    "1. **Accuracy Metrics**: Exact match, precision, recall, F1, Jaccard\n",
    "2. **Performance**: Compilation time, evaluation time\n",
    "3. **Cost**: API calls and token usage\n",
    "4. **Complexity**: Implementation difficulty\n",
    "\n",
    "### Goal:\n",
    "Identify the best optimizer for production deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cba502",
   "metadata": {},
   "source": [
    "### 10.1 Create Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feadbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä COMPREHENSIVE OPTIMIZER COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Collect all metrics\n",
    "comparison_data = [baseline_metrics]\n",
    "\n",
    "if labeledfewshot_module:\n",
    "    comparison_data.append(labeledfewshot_metrics)\n",
    "if gepa_module:\n",
    "    comparison_data.append(gepa_metrics)\n",
    "if bootstrap_module:\n",
    "    comparison_data.append(bootstrap_metrics)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Reorder columns for clarity\n",
    "column_order = ['optimizer', 'accuracy', 'precision', 'recall', 'f1', 'jaccard', \n",
    "                'compilation_time', 'evaluation_time', 'total_examples']\n",
    "comparison_df = comparison_df[[col for col in column_order if col in comparison_df.columns]]\n",
    "\n",
    "print(\"\\nüìà PERFORMANCE COMPARISON:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "comparison_path = OUTPUT_DIR / \"optimization_comparison.csv\"\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"\\n‚úÖ Saved comparison to: {comparison_path}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c410998",
   "metadata": {},
   "source": [
    "### 10.2 Visualize Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ace80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "ax1 = axes[0, 0]\n",
    "optimizers = comparison_df['optimizer'].tolist() if 'optimizer' in comparison_df else []\n",
    "accuracies = comparison_df['accuracy'].tolist() if 'accuracy' in comparison_df else []\n",
    "colors = ['#ff7f0e' if opt == 'Baseline' else '#1f77b4' for opt in optimizers]\n",
    "ax1.bar(optimizers, accuracies, color=colors)\n",
    "ax1.set_ylabel('Accuracy (Exact Match)')\n",
    "ax1.set_title('Accuracy Comparison')\n",
    "ax1.set_ylim([0, 1])\n",
    "for i, v in enumerate(accuracies):\n",
    "    ax1.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Precision, Recall, F1 comparison\n",
    "ax2 = axes[0, 1]\n",
    "x = np.arange(len(optimizers))\n",
    "width = 0.25\n",
    "if 'precision' in comparison_df and 'recall' in comparison_df and 'f1' in comparison_df:\n",
    "    ax2.bar(x - width, comparison_df['precision'], width, label='Precision', alpha=0.8)\n",
    "    ax2.bar(x, comparison_df['recall'], width, label='Recall', alpha=0.8)\n",
    "    ax2.bar(x + width, comparison_df['f1'], width, label='F1', alpha=0.8)\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title('Precision, Recall, F1 Comparison')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(optimizers, rotation=45)\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim([0, 1])\n",
    "\n",
    "# 3. Compilation time comparison\n",
    "ax3 = axes[1, 0]\n",
    "if 'compilation_time' in comparison_df:\n",
    "    comp_times = comparison_df['compilation_time'].fillna(0).tolist()\n",
    "    ax3.bar(optimizers, comp_times, color=['#2ca02c' if t < 60 else '#d62728' for t in comp_times])\n",
    "    ax3.set_ylabel('Time (seconds)')\n",
    "    ax3.set_title('Compilation Time')\n",
    "    ax3.set_xticklabels(optimizers, rotation=45)\n",
    "    for i, v in enumerate(comp_times):\n",
    "        if v > 0:\n",
    "            ax3.text(i, v + max(comp_times)*0.02, f'{v:.1f}s', ha='center')\n",
    "\n",
    "# 4. Accuracy improvement from baseline\n",
    "ax4 = axes[1, 1]\n",
    "if len(accuracies) > 0:\n",
    "    baseline_acc = accuracies[0]\n",
    "    improvements = [acc - baseline_acc for acc in accuracies]\n",
    "    colors_imp = ['#ff7f0e' if imp == 0 else ('#2ca02c' if imp > 0 else '#d62728') for imp in improvements]\n",
    "    ax4.bar(optimizers, improvements, color=colors_imp)\n",
    "    ax4.set_ylabel('Accuracy Improvement')\n",
    "    ax4.set_title('Improvement Over Baseline')\n",
    "    ax4.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax4.set_xticklabels(optimizers, rotation=45)\n",
    "    for i, v in enumerate(improvements):\n",
    "        ax4.text(i, v + 0.01 if v > 0 else v - 0.01, f'{v:+.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'optimization_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved visualization to: {OUTPUT_DIR / 'optimization_comparison.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0556a3",
   "metadata": {},
   "source": [
    "### 10.3 Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí∞ COST ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate approximate costs based on token usage\n",
    "# Note: Actual costs depend on your Azure pricing\n",
    "\n",
    "try:\n",
    "    total_cost = sum([x['cost'] for x in lm.history if x.get('cost') is not None])\n",
    "    print(f\"Total API cost (all optimizers): ${total_cost:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Based on LiteLLM cost tracking\")\n",
    "    \n",
    "    if len(comparison_df) > 1:\n",
    "        avg_cost_per_optimizer = total_cost / (len(comparison_df) - 1)  # Exclude baseline\n",
    "        print(f\"   ‚Ä¢ Average cost per optimizer: ${avg_cost_per_optimizer:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not calculate costs: {e}\")\n",
    "    print(\"   Cost tracking may not be available for your provider\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345a2ff",
   "metadata": {},
   "source": [
    "### 10.4 Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697bbbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí° RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find best performer\n",
    "if len(comparison_df) > 0:\n",
    "    best_accuracy_idx = comparison_df['accuracy'].idxmax()\n",
    "    best_optimizer = comparison_df.loc[best_accuracy_idx, 'optimizer']\n",
    "    best_accuracy = comparison_df.loc[best_accuracy_idx, 'accuracy']\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Overall Performance:\")\n",
    "    print(f\"   ‚Ä¢ Optimizer: {best_optimizer}\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {best_accuracy:.3f} ({best_accuracy*100:.1f}%)\")\n",
    "    \n",
    "    if 'f1' in comparison_df:\n",
    "        best_f1 = comparison_df.loc[best_accuracy_idx, 'f1']\n",
    "        print(f\"   ‚Ä¢ F1 Score: {best_f1:.3f}\")\n",
    "    \n",
    "    # Recommendations based on use case\n",
    "    print(f\"\\nüìã Use Case Recommendations:\")\n",
    "    print(f\"\\n   1. Quick Deployment (Low Cost, Fast):\")\n",
    "    print(f\"      ‚Üí LabeledFewShot\")\n",
    "    print(f\"      ‚Ä¢ Fastest compilation\")\n",
    "    print(f\"      ‚Ä¢ Lowest cost\")\n",
    "    print(f\"      ‚Ä¢ Good for prototypes\")\n",
    "    \n",
    "    print(f\"\\n   2. Best Performance (Higher Cost):\")\n",
    "    print(f\"      ‚Üí {best_optimizer}\")\n",
    "    print(f\"      ‚Ä¢ Highest accuracy: {best_accuracy:.3f}\")\n",
    "    print(f\"      ‚Ä¢ Best for production\")\n",
    "    \n",
    "    print(f\"\\n   3. Balanced Approach:\")\n",
    "    if labeledfewshot_module and gepa_module:\n",
    "        lfs_acc = labeledfewshot_metrics['accuracy']\n",
    "        gepa_acc = gepa_metrics['accuracy']\n",
    "        if abs(lfs_acc - gepa_acc) < 0.05:\n",
    "            print(f\"      ‚Üí LabeledFewShot (similar performance, faster)\")\n",
    "        else:\n",
    "            print(f\"      ‚Üí GEPA (better performance worth the cost)\")\n",
    "    \n",
    "    # Improvement summary\n",
    "    if baseline_metrics['accuracy'] < best_accuracy:\n",
    "        improvement = best_accuracy - baseline_metrics['accuracy']\n",
    "        print(f\"\\nüìà Overall Achievement:\")\n",
    "        print(f\"   ‚Ä¢ Improved accuracy by {improvement:.3f} ({improvement*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ From {baseline_metrics['accuracy']:.3f} to {best_accuracy:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b903fa7",
   "metadata": {},
   "source": [
    "## üìä Optimization Summary\n",
    "\n",
    "### ‚úÖ Completed Tasks\n",
    "\n",
    "1. ‚úÖ Loaded baseline results and data\n",
    "2. ‚úÖ Defined strict and soft evaluation metrics\n",
    "3. ‚úÖ Prepared DSPy training dataset\n",
    "4. ‚úÖ Applied LabeledFewShot optimizer\n",
    "5. ‚úÖ Applied GEPA optimizer\n",
    "6. ‚úÖ Applied BootstrapFewShot optimizer\n",
    "7. ‚úÖ Comprehensive comparison and analysis\n",
    "8. ‚úÖ Cost analysis and recommendations\n",
    "\n",
    "### üìà Performance Summary\n",
    "\n",
    "All optimization results are saved in:\n",
    "- `data/optimization_results/` - Predictions and metrics\n",
    "- `data/optimized_modules/` - Saved modules\n",
    "- `optimization_comparison.csv` - Side-by-side metrics\n",
    "- `optimization_comparison.png` - Visualization\n",
    "\n",
    "### üéØ Key Insights\n",
    "\n",
    "**Best Optimizer**: See recommendations above\n",
    "\n",
    "**Performance vs Cost Tradeoff**:\n",
    "- LabeledFewShot: Fast, cheap, decent performance\n",
    "- GEPA: Slow, expensive, best performance\n",
    "- BootstrapFewShot: Medium speed/cost, good performance\n",
    "\n",
    "### ‚û°Ô∏è Next Steps\n",
    "\n",
    "Proceed to **Notebook 4: Production Deployment** (`04_production_deployment.ipynb`) to:\n",
    "1. Load the best performing model\n",
    "2. Create production inference pipeline\n",
    "3. Set up monitoring and logging\n",
    "4. Deploy to Azure or local environment\n",
    "\n",
    "---\n",
    "\n",
    "**Prompt Optimization Complete!** üéâ\n",
    "\n",
    "You now have multiple optimized modules to choose from for production deployment."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
